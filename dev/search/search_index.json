{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DSS Deployment","text":""},{"location":"#introduction","title":"Introduction","text":"<p>An operational DSS deployment requires a specific architecture to be compliant with standards requirements and meet performance expectations as described in architecture.  This page describes the deployment procedures recommended by InterUSS to achieve this compliance and meet these expectations.</p>"},{"location":"#deployment-layers","title":"Deployment layers","text":"<p>This repository provides three layers of abstraction to deploy and operate a DSS instance via Kubernetes.</p> <p></p> <p>As described below, InterUSS provides tooling for Kubernetes deployments on Amazon Web Services (EKS) and Google Cloud (GKE). However, you can do this on any supported cloud provider or even on your own infrastructure. Review InterUSS pooling requirements and consult the Kubernetes documentation for your chosen provider.</p> <p>The three layers are the following:</p> <ol> <li> <p>Infrastructure provides instructions and tooling to easily provision a Kubernetes cluster and cloud resources (load balancers, storage...) to a cloud provider. The resulting infrastructure meets the Pooling requirements.    Terraform modules are provided for:</p> <ul> <li>Amazon Web Services (EKS)</li> <li>Google (GKE)</li> </ul> </li> <li> <p>Services provides the tooling to deploy a DSS instance to a Kubernetes cluster.</p> <ul> <li>Helm Charts</li> <li>Tanka</li> </ul> </li> <li> <p>Operations provides instructions to operate a deployed DSS instance.</p> <ul> <li>Pooling procedure</li> <li>Troubleshooting</li> </ul> </li> </ol> <p>Depending on your level of expertise and your internal organizational practices, you should be able to use each layer independently or complementary.</p> <p>For local deployment approaches, see the documentation located in the build folder</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>You can find below two guides to deploy a DSS instance from scratch: - Amazon Web Services (EKS) - Google (GKE)</p> <p>For a complete use case, you can look into the configurations of the CI job in operations: ci</p>"},{"location":"#migrations-and-upgrades","title":"Migrations and upgrades","text":"<p>Information related to migrations and upgrades can be found in MIGRATION.md.</p>"},{"location":"#development","title":"Development","text":"<p>The following diagram represents the resources in this repository per layer. </p>"},{"location":"#formatting","title":"Formatting","text":"<p>Terraform files must be formatted using <code>terraform fmt -recursive</code> command to pass the CI linter check.</p>"},{"location":"architecture/","title":"Kubernetes deployment","text":""},{"location":"architecture/#introduction","title":"Introduction","text":"<p>See introduction</p>"},{"location":"architecture/#architecture","title":"Architecture","text":"<p>The expected deployment configuration of a DSS pool supporting a DSS Region is multiple organizations to each host one DSS instance that is interoperable with each other organization's DSS instance.  A DSS pool with three participating organizations (USSs) will have an architecture similar to the diagram below.</p> <p>Note that the diagram shows 2 stateful sets per DSS instance.  Currently, the helm and tanka deployments produce 3 stateful sets per DSS instance.  However, after Issue #481 is resolved, this is expected to be reduced to 2 stateful sets.</p> <p></p>"},{"location":"architecture/#terminology-notes","title":"Terminology notes","text":"<p>See teminology notes.</p>"},{"location":"architecture/#pooling","title":"Pooling","text":""},{"location":"architecture/#objective","title":"Objective","text":"<p>See Pooling Objective and subsections.</p>"},{"location":"architecture/#additional-requirements","title":"Additional requirements","text":"<p>See Additional requirements.</p>"},{"location":"architecture/#survivability","title":"Survivability","text":"<p>One of the primary design considerations of the DSS is to be very resilient to failures.  This resiliency is obtained primarily from the behavior of the underlying CockroachDB database technology and how we configure it.  The diagram below shows the result of failures (bringing a node down for maintenance, or having an entire USS go down) from different starting points, assuming 3 replicas.</p> <p></p> <p>The table below summarizes survivable failures with 3 DSS instances configured according to the architecture described above.  Each system state is summarized by three groups (one group per USS) of two nodes per USS.</p> <ul> <li>\ud83d\udfe9 : Functional node has no recent changes in functionality</li> <li>\ud83d\udfe5 : Non-functional node in down USS has no recent changes in functionality</li> <li>\ud83d\udfe7 : Non-functional node due to USS upgrade or maintenance has no recent changes in functionality</li> <li>\ud83d\udd34 : Node becomes non-functional due to a USS going down</li> <li>\ud83d\udfe0 : Node becomes non-functional due to USS upgrade or maintenance</li> </ul> Pre-existing conditions New failures Survivable? (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe0 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) \ud83d\udd34 No; some ranges may be lost because of this bug (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) \ud83d\udd34 No; some ranges may be lost (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udd34 No; ranges guaranteed to be lost (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udd34 No; some ranges may be lost because of this bug (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udfe2 Yes (\ud83d\udd34 , \ud83d\udd34 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) \ud83d\udd34 No; ranges guaranteed to be lost (\ud83d\udfe0 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) \ud83d\udd34 No; ranges guaranteed to be lost (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe5 , \ud83d\udfe5 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe5 , \ud83d\udfe5 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe5 , \ud83d\udfe5 ) \ud83d\udd34 No; some ranges may be lost (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) (\ud83d\udfe5 , \ud83d\udfe5 ) \ud83d\udd34 No; some ranges may be lost"},{"location":"architecture/#sizing","title":"Sizing","text":""},{"location":"architecture/#introduction_1","title":"Introduction","text":"<p>This section contains an estimate of the computational and other resources likely necessary to support expected demand in a country similar to the United States.</p>"},{"location":"architecture/#time-required-to-fulfill-queries-for-a-single-flight","title":"Time required to fulfill queries for a single flight","text":"<ol> <li>Assume 1 ISA per flight (worst case)<ol> <li>2 ISA management queries per flight (create &amp; delete)</li> </ol> </li> <li>Assume 90% of flights are nominal and require 3 strategic deconfliction queries (Accepted, Activated, Ended) while 10% of flights have problems and require 7 strategic deconfliction queries<ol> <li>3.4 strategic deconfliction queries per flight</li> </ol> </li> <li>Assume 0.1 seconds to fulfill a query<ol> <li>Therefore, 0.54 seconds required (on average) to fulfill management queries to support a flight</li> </ol> </li> </ol>"},{"location":"architecture/#time-required-to-fulfill-queries-for-a-rid-display-provider","title":"Time required to fulfill queries for a RID Display Provider","text":"<ol> <li>Assume 2 Display Providers viewing each flight on average, 4 subscriptions per flight per DP, and 40% chance of subscription reuse<ol> <li>9.6 subscription queries per flight</li> <li>0.96 seconds required (on average) to fulfill viewing queries to support a flight</li> </ol> </li> </ol>"},{"location":"architecture/#required-parallelism","title":"Required parallelism","text":"<ol> <li>Use 348,537 remote pilots in 2024</li> <li>Assume 100 flights per month per remote pilot</li> <li>Use 989,916 recreational pilots as a baseline (even though this is likely number of aircraft, not number of pilots) and double it for the future</li> <li>Use 7.1 flights per month per recreational pilot</li> <li>Therefore, expect about 18.6 flights per second</li> <li>With 1.5 seconds of query time per flight, a nominal parallelism of 28 is required to satisfy the demand</li> <li>Assuming a peak-average ratio of 3.5, a parallelism of 98 is required</li> </ol>"},{"location":"architecture/#required-resources","title":"Required resources","text":"<ol> <li>With Cockroach Labs guidance of 4 parallel operations per vCPU, the DSS pool requires 25 vCPUs.</li> <li>Assuming 3 DSS instances and the need to continue to operate when one instance is down, each DSS instance requires 13 vCPUs.</li> <li>Using 8-vCPU virtual machines (like n2-standard-8), this means each instance needs 2 of these virtual machines</li> <li>Assuming that 5 days' worth of flights are occupying space on disk at any given time and that each flight record on disk is 100k, approximately 83 GB of storage is required<ol> <li>Note that Cockroach Labs recommends 4,000 read IO/s and 4,000 write IO/s, and some cloud providers scale storage speed with storage size, so 83 GB of storage may be far less than is necessary to achieve these speed numbers</li> </ol> </li> </ol>"},{"location":"build/","title":"Deploying a DSS instance","text":""},{"location":"build/#deployment-options","title":"Deployment options","text":"<p>This document describes how to deploy a production-style DSS instance to interoperate with other DSS instances in a DSS pool.</p> <p>To run a local DSS instance for testing, evaluation, or development, see dev/standalone_instance.md.</p> <p>To create a local DSS instance with multi-node CRDB cluster, see dev/mutli_node_local_dss.md.</p> <p>To create or join a pool consisting of multiple interoperable DSS instances, see information on pooling.</p>"},{"location":"build/#glossary","title":"Glossary","text":"<ul> <li>DSS Region - A region in which a single, unified airspace representation is   presented by one or more interoperable DSS instances, each instance typically   operated by a separate organization.  A specific environment (for example,   \"production\" or \"staging\") in a particular DSS Region is called a \"pool\".</li> <li>DSS instance - a single logical replica in a DSS pool.</li> </ul>"},{"location":"build/#preface","title":"Preface","text":"<p>This doc describes a procedure for deploying the DSS and its dependencies (namely CockroachDB) via Kubernetes. The use of Kubernetes is not a requirement, and a DSS instance can join a CRDB cluster constituting a DSS pool as long as it meets the CockroachDB requirements below.</p>"},{"location":"build/#prerequisites","title":"Prerequisites","text":"<p>Download &amp; install the following tools to your workstation:</p> <ul> <li>If deploying on Google Cloud,   install Google Cloud SDK</li> <li>Confirm successful installation with <code>gcloud version</code></li> <li>Run <code>gcloud init</code> to set up a connection to your account.</li> <li><code>kubectl</code> can be installed from <code>gcloud</code> instead of via the method below.</li> <li>Install kubectl to   interact with kubernetes</li> <li>Confirm successful installation with <code>kubectl version --client</code> (should     succeed from any working directory).</li> <li>Note that kubectl can alternatively be installed via the Google Cloud SDK    <code>gcloud</code> shell if using Google Cloud.</li> <li>Install tanka</li> <li>On Linux, after downloading the binary per instructions, run     <code>sudo chmod +x /usr/local/bin/tk</code></li> <li>Confirm successful installation with <code>tk --version</code></li> <li>Install Docker.</li> <li>Confirm successful installation with <code>docker --version</code></li> <li>Install CockroachDB to   generate CockroachDB certificates.</li> <li>These instructions assume CockroachDB Core.</li> <li>You may need to run <code>sudo chmod +x /usr/local/bin/cockroach</code> after     completing the installation instructions.</li> <li>Confirm successful installation with <code>cockroach version</code></li> <li>If developing the DSS codebase,   install Golang</li> <li>Confirm successful installation with <code>go version</code></li> <li>Optionally install Jsonnet if editing   the jsonnet templates.</li> </ul>"},{"location":"build/#docker-images","title":"Docker images","text":"<p>The application logic of the DSS is located in core-service which is provided in a Docker image which is built locally and then pushed to a Docker registry of your choice.  All major cloud providers have a docker registry service, or you can set up your own.</p> <p>To use the prebuilt InterUSS Docker images (without building them yourself), use <code>docker.io/interuss/dss</code> for <code>VAR_DOCKER_IMAGE_NAME</code>.</p> <p>To build these images (and, optionally, push them to a docker registry):</p> <ol> <li> <p>Set the environment variable <code>DOCKER_URL</code> to your docker registry url endpoint.</p> <ul> <li> <p>For Google Cloud, <code>DOCKER_URL</code> should be set similarly to as described     here,     like <code>gcr.io/your-project-id</code> (do not include the image name;     it will be appended by the build script)</p> </li> <li> <p>For Amazon Web Services, <code>DOCKER_URL</code> should be set similarly to as described     here,     like <code>${aws_account_id}.dkr.ecr.${region}.amazonaws.com/</code> (do not include the image name;     it will be appended by the build script)</p> </li> </ul> </li> <li> <p>Ensure you are logged into your docker registry service.</p> <ul> <li> <p>For Google Cloud,     these     are the recommended instructions (<code>gcloud auth configure-docker</code>).     Ensure that     appropriate permissions are enabled.</p> </li> <li> <p>For Amazon Web Services, create a private repository by following the instructions     here, then login     as described here.</p> </li> </ul> </li> <li> <p>Use the <code>build.sh</code> script in this directory to build and push    an image tagged with the current date and git commit hash.</p> </li> <li> <p>Note the VAR_* value printed at the end of the script.</p> </li> </ol>"},{"location":"build/#access-to-private-repository","title":"Access to private repository","text":"<p>See below the description of <code>VAR_DOCKER_IMAGE_PULL_SECRET</code> to configure authentication.</p>"},{"location":"build/#verify-signature-of-prebuilt-interuss-docker-images","title":"Verify signature of prebuilt InterUSS Docker images","text":"<p>The prebuilt docker images are signed using sigstore. The identity of the CI workflow, attested by GitHub, is used so sign the images.</p> <p>The signature may be verified by using cosign: <pre><code>docker pull \"docker.io/interuss/dss:latest\"\ncosign verify \"docker.io/interuss/dss:latest\" \\\n  --certificate-identity-regexp=\"https://github.com/interuss/dss/.github/workflows/dss-publish.yml@refs/*\" \\\n  --certificate-oidc-issuer=\"https://token.actions.githubusercontent.com\"\n</code></pre> Adapt the version specified if required.</p>"},{"location":"build/#deploying-a-dss-instance-via-kubernetes","title":"Deploying a DSS instance via Kubernetes","text":"<p>This section discusses deploying a Kubernetes service manually, although you can deploy a DSS instance however you like as long as it meets the CockroachDB requirements above. You can do this on any supported cloud provider or even on your own infrastructure. Consult the Kubernetes documentation for your chosen provider.</p> <p>To instead deploy infrastructure using terraform, see the terraform infrastructure deployment page.</p> <p>If you can augment this documentation with specifics for another cloud provider, a PR to that effect would be greatly appreciated.</p> <ol> <li> <p>Create a new Kubernetes cluster. We recommend a new cluster for each DSS     instance.  A reasonable cluster name might be <code>dss-us-prod-e4a</code> (where <code>e4a</code>     is a zone identifier abbreviation), <code>dss-ca-staging</code>,     <code>dss-mx-integration-sae1a</code>, etc.  The name of this cluster will be combined     with other information by Kubernetes to generate a longer cluster context     ID.</p> <ul> <li>On Google Cloud, the recommended procedure to create a cluster is:</li> <li>In Google Cloud Platform, go to the Kubernetes Engine page and under       Clusters click Create cluster.</li> <li>Name the cluster appropriately; e.g., <code>dss-us-prod</code></li> <li>Select Zonal and a compute-zone appropriate to your       geography</li> <li>For the \"default-pool\" node pool:<ul> <li>Enter 3 for number of nodes.</li> <li>In the \"Nodes\" bullet under \"default-pool\", select N2 series and      n2-standard-4 for machine type.</li> </ul> </li> <li>In the \"Networking\" bullet under \"Clusters\", ensure \"Enable VPC       -native traffic\"       is checked.</li> </ul> </li> <li> <p>Make sure correct cluster context is selected by printing the context     name to the console: <code>kubectl config current-context</code></p> <ul> <li> <p>Record this value and use it for <code>$CLUSTER_CONTEXT</code> below; perhaps:    <code>export CLUSTER_CONTEXT=$(kubectl config current-context)</code></p> </li> <li> <p>On Google Cloud, first configure kubectl to interact with the cluster   created above with   these instructions.   Specifically:</p> </li> <li><code>gcloud config set project your-project-id</code></li> <li><code>gcloud config set compute/zone your-compute-zone</code></li> <li><code>gcloud container clusters get-credentials your-cluster-name</code></li> </ul> </li> <li> <p>Ensure the desired namespace is selected; the recommended     namespace is simply <code>default</code> with one cluster per DSS instance.  Print the     the current namespaces with <code>kubectl get namespace</code>.  Use the current     namespace as the value for <code>$NAMESPACE</code> below; perhaps use an environment     variable for convenience: <code>export NAMESPACE=&lt;your namespace&gt;</code>.</p> <p>It may be useful to create a <code>login.sh</code> file with content like that shown below and <code>source login.sh</code> when working with this cluster.</p> <p>GCP: <pre><code>#!/bin/bash\n\nexport CLUSTER_NAME=&lt;your cluster name&gt;\nexport REGION=&lt;GCP region in which your cluster resides&gt;\ngcloud config set project &lt;your GCP project name&gt;\ngcloud config set compute/zone $REGION-a\ngcloud container clusters get-credentials $CLUSTER_NAME\nexport CLUSTER_CONTEXT=$(kubectl config current-context)\nexport NAMESPACE=default\nexport DOCKER_URL=docker.io/interuss\necho \"Current CLUSTER_CONTEXT is $CLUSTER_CONTEXT\n</code></pre></p> </li> <li> <p>Create static IP addresses: one for the Core Service ingress, and one     for each CockroachDB node if you want to be able to interact with other     DSS instances.</p> <ul> <li> <p>If using Google Cloud, the Core Service ingress needs to be created as    a \"Global\" IP address, but the CRDB ingresses as \"Regional\" IP addresses.    IPv4 is recommended as IPv6 has not yet been tested.  Follow    these instructions    to reserve the static IP addresses.  Specifically (replacing    CLUSTER_NAME as appropriate since static IP addresses are defined at    the project level rather than the cluster level), e.g.:</p> <ul> <li><code>gcloud compute addresses create ${CLUSTER_NAME}-backend --global --ip-version IPV4</code></li> <li><code>gcloud compute addresses create ${CLUSTER_NAME}-crdb-0 --region $REGION</code></li> <li><code>gcloud compute addresses create ${CLUSTER_NAME}-crdb-1 --region $REGION</code></li> <li><code>gcloud compute addresses create ${CLUSTER_NAME}-crdb-2 --region $REGION</code></li> </ul> </li> </ul> </li> <li> <p>Link static IP addresses to DNS entries.</p> <ul> <li> <p>Your CockroachDB nodes should have a common hostname suffix; e.g.,    <code>*.db.interuss.com</code>.  Recommended naming is    <code>0.db.yourdeployment.yourdomain.com</code>,    <code>1.db.yourdeployment.yourdomain.com</code>, etc.</p> </li> <li> <p>If using Google Cloud, see    these instructions    to create DNS entries for the static IP addresses created above.  To list    the IP addresses, use <code>gcloud compute addresses list</code>.</p> </li> </ul> </li> <li> <p>(Only if you use CockroachDB) Use <code>make-certs.py</code> script to create certificates for     the CockroachDB nodes in this DSS instance:</p> <pre><code>./make-certs.py --cluster $CLUSTER_CONTEXT --namespace $NAMESPACE\n    [--node-address &lt;ADDRESS&gt; &lt;ADDRESS&gt; &lt;ADDRESS&gt; ...]\n    [--ca-cert-to-join &lt;CA_CERT_FILE&gt;]\n</code></pre> <ol> <li> <p><code>$CLUSTER_CONTEXT</code> is the name of the cluster (see step 2 above).</p> </li> <li> <p><code>$NAMESPACE</code> is the namespace for this DSS instance (see step 3 above).</p> </li> <li> <p><code>Each ADDRESS</code> is the DNS entry for a CockroachDB node that will use the     certificates generated by this command.  This is usually just the nodes     constituting this DSS instance, though if you maintain multiple DSS     instances in a single pool, the separate instances may share     certificates.  Note that <code>--node-address</code> must include all the hostnames     and/or IP addresses that other CockroachDB nodes will use to connect to     your nodes (the nodes using these certificates). Wildcard notation is     supported, so you can use <code>*.&lt;subdomain&gt;.&lt;domain&gt;.com&gt;</code>.  If following     the recommendations above, use a single ADDRESS similar to     <code>*.db.yourdeployment.yourdomain.com</code>.  The ADDRESS entries should be     separated by spaces.</p> </li> <li> <p>If you are pooling with existing DSS instance(s) you need their CA     public cert (ca.crt), which will be concatenated with yours. Set     <code>--ca-cert-to-join</code> to a <code>ca.crt</code> file.  Reach out to existing operators     to request their public cert.  If not joining an existing pool, omit     this argument.</p> </li> <li> <p>Note: If you are creating multiple DSS instances at once, and joining     them together you likely want to copy the nth instance's <code>ca.crt</code> into     the rest of the instances, such that ca.crt is the same across all     instances.</p> </li> </ol> </li> <li> <p>(Only if you use Yugabyte) Use <code>dss-certs.py</code> script to create certificates for the Yugabyte nodes in this DSS instance.</p> </li> <li> <p>If joining an existing DSS pool, share ca.crt with the DSS instance(s) you     are trying to join, and have them apply the new ca.crt, which now contains     both your instance's and the original instance's public certs, to enable     secure bi-directional communication.  Each original DSS instance, upon     receipt of the combined ca.crt from the joining instance, should perform the     actions below.  While they are performing those actions, you may continue     with the instructions.</p> <ol> <li> <p>If you use CockroachDB:</p> <ol> <li>Overwrite its existing ca.crt with the new ca.crt provided by the DSS instance joining the pool.</li> <li>Upload the new ca.crt to its cluster using <code>./apply-certs.sh $CLUSTER_CONTEXT $NAMESPACE</code></li> <li>Restart their CockroachDB pods to recognize the updated ca.crt: <code>kubectl rollout restart statefulset/cockroachdb --namespace $NAMESPACE</code></li> <li>Inform you when their CockroachDB pods have finished restarting (typically around 10 minutes)</li> </ol> </li> <li> <p>If you use Yugabyte</p> <ol> <li>Share your CA with <code>./dss-certs.py get-ca</code></li> <li>Add others CAs of the pool with <code>./dss-certs.py add-pool-ca</code></li> <li>Upload the new CAs to its cluster using <code>./dss-certs.py apply</code></li> <li>Restart their Yugabyte pods to recognize the updated ca.crt: <code>kubectl rollout restart statefulset/yb-master --namespace $NAMESPACE</code> <code>kubectl rollout restart statefulset/yb-tserver --namespace $NAMESPACE</code></li> <li>Inform you when their Yugabyte pods have finished restarting (typically around 10 minutes)</li> </ol> </li> </ol> </li> <li> <p>Ensure the Docker images are built according to the instructions in the     previous section.</p> </li> <li> <p>From this working directory,     <code>cp -r ../deploy/services/tanka/examples/minimum/* workspace/$CLUSTER_CONTEXT</code>.  Note that     the <code>workspace/$CLUSTER_CONTEXT</code> folder should have already been created     by the <code>make-certs.py</code> script.     Replace the imports at the top of <code>main.jsonnet</code> to correctly locate the files:     <pre><code>local dss = import '../../../deploy/services/tanka/dss.libsonnet';\nlocal metadataBase = import '../../../deploy/services/tanka/metadata_base.libsonnet';\n</code></pre></p> </li> <li> <p>If providing a .pem file directly as the public key to validate incoming     access tokens, copy it to dss/build/jwt-public-certs.     Public key specification by JWKS is preferred; if using the JWKS approach     to specify the public key, skip this step.</p> </li> <li> <p>Edit <code>workspace/$CLUSTER_CONTEXT/main.jsonnet</code> and replace all <code>VAR_*</code>     instances with appropriate values:</p> <ol> <li> <p><code>VAR_NAMESPACE</code>: Same <code>$NAMESPACE</code> used in the make-certs.py (and     apply-certs.sh) scripts.</p> </li> <li> <p><code>VAR_CLUSTER_CONTEXT</code>: Same $CLUSTER_CONTEXT used in the <code>make-certs.py</code>     and <code>apply-certs.sh</code> scripts.</p> </li> <li> <p><code>VAR_ENABLE_SCD</code>: Set this boolean true to enable strategic conflict     detection functionality (currently an R&amp;D project tracking an initial     draft of the upcoming ASTM standard).</p> </li> <li> <p><code>VAR_LOCALITY</code>: Unique name for your DSS instance.  Currently, we     recommend \"_\", and the <code>=</code> character is not     allowed.  However, any unique (among all other participating DSS     instances) value is acceptable. <li> <p><code>VAR_DB_HOSTNAME_SUFFIX</code>: The domain name suffix shared by all of your     CockroachDB nodes.  For instance, if your CRDB nodes were addressable at     <code>0.db.example.com</code>, <code>1.db.example.com</code>, and <code>2.db.example.com</code>, then     VAR_DB_HOSTNAME_SUFFIX would be <code>db.example.com</code>.</p> </li> <li> <p><code>VAR_DATASTORE</code>: Datastore to use. Can be set to 'cockroachdb' or 'yugabyte'.</p> </li> <li> <p><code>VAR_CRDB_DOCKER_IMAGE_NAME</code>: Docker image of cockroach db pods. Until     DSS v0.16, the recommended CockroachDB image name is <code>cockroachdb/cockroach:v21.2.7</code>.     From DSS v0.17, the recommended CockroachDB version is <code>cockroachdb/cockroach:v24.1.3</code>.</p> </li> <li> <p><code>VAR_CRDB_NODE_IPn</code>: IP address (numeric) of nth CRDB node (add more     entries if you have more than 3 CRDB nodes).  Example: <code>1.1.1.1</code></p> </li> <li> <p><code>VAR_SHOULD_INIT</code>: Set to <code>false</code> if joining an existing pool, <code>true</code>     if creating the first DSS instance for a pool.  When set <code>true</code>, this     can initialize the data directories on your cluster, and prevent you     from joining an existing pool.</p> </li> <li> <p><code>VAR_EXTERNAL_CRDB_NODEn</code>: Fully-qualified domain name of existing CRDB     nodes if you are joining an existing pool.  If more than three are     available, add additional entries.  If not joining an existing pool,     comment out this <code>JoinExisting:</code> line.</p> <ul> <li>You should supply a minimum of 3 seed nodes to every CockroachDB node.   These 3 nodes should be the same for every node (ie: every node points   to node 0, 1, and 2). For external DSS instances you should point to a   minimum of 3, or you can use a loadbalanced hostname or IP address of   other DSS instances. You should do this for every DSS instance in the   pool, including newly joined instances. See CockroachDB's note on the   join flag.</li> </ul> </li> <li> <p><code>VAR_YUGABYTE_DOCKER_IMAGE_NAME</code>: Docker image of Yugabyte db pods.     Shall be set to at least <code>yugabytedb/yugabyte:2.25.1.0-b381</code></p> </li> <li> <p><code>VAR_YUGABYTE_MASTER_IPn</code>: IP address (numeric) of nth Yugabyte     master node (add more entries if you have more than 3 nodes).     Example: <code>1.1.1.1</code></p> </li> <li> <p><code>VAR_YUGABYTE_TSERVER_IPn</code>: IP address (numeric) of nth Yugabyte     tserver node (add more entries if you have more than 3 nodes).     Example: <code>1.1.1.1</code></p> </li> <li> <p><code>VAR_YUGABYTE_MASTER_ADDRESSn</code>: List of addresses of Yugabyte master     nodes in the DSS pool. Must be accessible from all master/tserver nodes     and identical in a cluster. Example: <code>[\"0.master.db.uss1.example.com\", \"1.master.db.uss1.example.com\", \"3.master.db.uss1.example.com\", \"0.master.db.uss2.example.com\", \"1.master.db.uss2.example.com\", \"3.master.db.uss2.example.com\"]</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_MASTER_RPC_BIND_ADDRESSES</code>: Bind address for yugabyte     master node. May use <code>${HOSTNAME}</code>, <code>${NAMESPACE}</code> or <code>${HOSTNAMENO}</code>     to use respectively hostname, namespace or number of the node.     Example: <code>${HOSTNAMENO}.master.db.uss1.example.com</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_MASTER_BROADCAST_ADDRESSES</code>: Broadcast address for yugabyte     master node. May use <code>${HOSTNAME}</code>, <code>${NAMESPACE}</code> or <code>${HOSTNAMENO}</code>     to use respectively hostname, namespace or number of the node.     Example: <code>${HOSTNAMENO}.master.db.uss1.example.com:7100</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_TSERVER_RPC_BIND_ADDRESSES</code>: Bind address for yugabyte     tserver node. May use <code>${HOSTNAME}</code>, <code>${NAMESPACE}</code> or <code>${HOSTNAMENO}</code>     to use respectively hostname, namespace or number of the node.     Example: <code>${HOSTNAMENO}.tserver.db.uss1.example.com</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_TSERVER_BROADCAST_ADDRESSES</code>: Broadcast address for yugabyte     tserver node. May use <code>${HOSTNAME}</code>, <code>${NAMESPACE}</code> or <code>${HOSTNAMENO}</code>     to use respectively hostname, namespace or number of the node.     Example: <code>${HOSTNAMENO}.tserver.db.uss1.example.com:9100</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_FIX_27367_ISSUE</code>: Fix issue 27367     To make the fix working, RPC bind and broadcast addresses must be set to     the same, public value on where the master / tserver node is accessible.</p> </li> <li> <p><code>VAR_YUGABYTE_LIGHT_RESOURCES</code>: Use light resources in term of CPU/Memory     for Yugabyte nodes. You may use that for development purposes, to deploy     a Yugabyte in a small cluster to save costs and resources.</p> </li> <li> <p><code>VAR_YUGABYTE_PLACEMENT_CLOUD</code>: Yugabyte placement's cloud value, for     master and tserver nodes.     Example: <code>cloud-1</code></p> </li> <li> <p><code>VAR_YUGABYTE_PLACEMENT_REGION</code>: Yugabyte placement's region value, for     master and tserver nodes.     Example: <code>uss-1</code></p> </li> <li> <p><code>VAR_YUGABYTE_PLACEMENT_ZONE</code>: Yugabyte placement's zone value, for     master and tserver nodes.     Example: <code>zone-1</code></p> </li> <li> <p><code>VAR_STORAGE_CLASS</code>: Kubernetes Storage Class to use for CockroachDB,     Yugabyte and Prometheus volumes. You can check your cluster's possible     values with <code>kubectl get storageclass</code>. If you're not sure, each cloud     provider has some default storage classes that should work:</p> <ul> <li>Google Cloud: <code>standard</code></li> <li>Azure: <code>default</code></li> <li>AWS: <code>gp2</code></li> </ul> </li> <li> <p><code>VAR_INGRESS_NAME</code>: If using Google Kubernetes Engine, set this to the     the name of the core-service static IP address created above (e.g.,     <code>CLUSTER_NAME-backend</code>).</p> </li> <li> <p><code>VAR_DOCKER_IMAGE_NAME</code>: Full name of the docker image built in the     section above.  <code>build.sh</code> prints this name as the last thing it does     when run with <code>DOCKER_URL</code> set.  It should look something like     <code>gcr.io/your-project-id/dss:2020-07-01-46cae72cf</code> if you built the image     yourself, or <code>docker.io/interuss/dss</code> if using the InterUSS image     without <code>build.sh</code>.</p> <ul> <li>Note that <code>VAR_DOCKER_IMAGE_NAME</code> is used in two places.</li> </ul> </li> <li> <p><code>VAR_DOCKER_IMAGE_PULL_SECRET</code>: Secret name of the credentials to access     the image registry. If the image specified in VAR_DOCKER_IMAGE_NAME does not require     authentication to be pulled, then do not populate this instance and do not uncomment     the line containing it. You can use the following command to store the credentials     as kubernetes secret:</p> <p>kubectl create secret -n VAR_NAMESPACE docker-registry VAR_DOCKER_IMAGE_PULL_SECRET \\     --docker-server=DOCKER_REGISTRY_SERVER \\     --docker-username=DOCKER_USER \\     --docker-password=DOCKER_PASSWORD \\     --docker-email=DOCKER_EMAIL</p> <p>For docker hub private repository, use <code>docker.io</code> as <code>DOCKER_REGISTRY_SERVER</code> and an access token as <code>DOCKER_PASSWORD</code>.</p> </li> <li> <p><code>VAR_APP_HOSTNAME</code>: Fully-qualified domain name of your Core Service     ingress endpoint.  For example, <code>dss.example.com</code>.</p> </li> <li> <p><code>VAR_PUBLIC_ENDPOINT</code>: URL to publicly access your Core Service     ingress endpoint.  For example, <code>https://dss.example.com</code>. Only for versions &gt;=0.21.</p> </li> <li> <p><code>VAR_PUBLIC_KEY_PEM_PATH</code>: If providing a .pem file directly as the     public key to validate incoming access tokens, specify the name of this     .pem file here as <code>/jwt-public-certs/YOUR-KEY-NAME.pem</code> replacing     YOUR-KEY-NAME as appropriate.  For instance, if using the provided     <code>us-demo.pem</code>, use the path     <code>/jwt-public-certs/us-demo.pem</code>.  Note that your .pem file must have     been copied into <code>jwt-public-certs</code> in an earlier     step, or mounted at runtime using a volume.</p> <ul> <li>If providing an access token public key via JWKS, provide a blank   string for this parameter.</li> </ul> </li> <li> <p><code>VAR_JWKS_ENDPOINT</code>: If providing the access token public key via JWKS,     specify the JWKS endpoint here.  Example:     <code>https://auth.example.com/.well-known/jwks.json</code></p> <ul> <li>If providing a .pem file directly as the public key to valid incoming access tokens, provide a blank string for this parameter.</li> </ul> </li> <li> <p><code>VAR_JWKS_KEY_ID</code>: If providing the access token public key via JWKS,     specify the <code>kid</code> (key ID) of they appropriate key in the JWKS file     referenced above.</p> <ul> <li>If providing a .pem file directly as the public key to valid incoming access tokens, provide a blank string for this parameter.</li> </ul> </li> <li> <p>If you are only turning up a single DSS instance for development, you     may optionally change <code>single_cluster</code> to <code>true</code>.</p> </li> <li> <p><code>VAR_SSL_POLICY</code>: When deploying on Google Cloud, a ssl policy     can be applied to the DSS Ingress. This can be used to secure the TLS connection.     Follow the instructions to create the Global SSL Policy and     replace VAR_SSL_POLICY variable with its name. <code>RESTRICTED</code> profile is recommended.     Leave it empty if not applicable.</p> </li> <li> <p><code>VAR_ENABLE_SCHEMA_MANAGER</code>: Set this to true to enable the schema manager jobs.     It is required to perform schema upgrades. Note that it is automatically enabled when <code>VAR_SHOULD_INIT</code> is true.</p> </li> <li> <p>Edit workspace/$CLUSTER_CONTEXT/spec.json and replace all VAR_*     instances with appropriate values:</p> <ol> <li> <p>VAR_API_SERVER: Determine this value with the command:</p> <p><code>echo $(kubectl config view -o jsonpath=\"{.clusters[?(@.name==\\\"$CLUSTER_CONTEXT\\\")].cluster.server}\")</code></p> <ul> <li>Note that <code>$CLUSTER_CONTEXT</code> should be replaced with your actual  <code>CLUSTER_CONTEXT</code> value prior to executing the above command if you  have not defined a <code>CLUSTER_CONTEXT</code> environment variable.</li> </ul> </li> <li> <p>VAR_NAMESPACE: See previous section.</p> </li> </ol> </li> <li> <p>Use the <code>apply-certs.sh</code> script to create secrets on the     Kubernetes cluster containing the certificates and keys generated in the     previous step.</p> <pre><code>./apply-certs.sh $CLUSTER_CONTEXT $NAMESPACE\n</code></pre> </li> <li> <p>Run <code>tk apply workspace/$CLUSTER_CONTEXT</code> to apply it to the     cluster.</p> <ul> <li>If you are joining an existing pool, do not execute this command until the   the existing DSS instances all confirm that their CockroachDB pods have   finished their rolling restarts.</li> </ul> </li> <li> <p>Wait for services to initialize.  Verify that basic services are functioning     by navigating to https://your-domain.example.com/healthy.</p> <ul> <li>On Google Cloud, the highest-latency operation is provisioning of the   HTTPS certificate which generally takes 10-45 minutes.  To track this   progress:</li> <li>Go to the \"Services &amp; Ingress\" left-side tab from the Kubernetes Engine     page.</li> <li>Click on the <code>https-ingress</code> item (filter by just the cluster of     interest if you have multiple clusters in your project).</li> <li>Under the \"Ingress\" section for Details, click on the link corresponding     with \"Load balancer\".</li> <li>Under Frontend for Details, the Certificate column for HTTPS protocol     will have an icon next to it which will change to a green checkmark when     provisioning is complete.</li> <li>Click on the certificate link to see provisioning progress.</li> <li>If everything indicates OK and you still receive a cipher mismatch error     message when attempting to visit /healthy, wait an additional 5 minutes     before attempting to troubleshoot further.</li> </ul> </li> <li> <p>If joining an existing pool, share your CRDB node addresses with the     operators of the existing DSS instances.  They will add these node addresses     to JoinExisting where <code>VAR_CRDB_EXTERNAL_NODEn</code> is indicated in the minimum     example, and then update their deployment:</p> <p><code>tk apply workspace/$CLUSTER_CONTEXT</code></p> </li>"},{"location":"build/#pooling","title":"Pooling","text":"<p>See the pooling documentation.</p>"},{"location":"build/#tools","title":"Tools","text":"<p>See operations monitoring documentation.</p>"},{"location":"build/#troubleshooting","title":"Troubleshooting","text":"<p>See Troubleshooting in <code>deploy/operations</code>.</p>"},{"location":"build/#upgrading-database-schemas","title":"Upgrading Database Schemas","text":"<p>All schemas-related files are in <code>db_schemas</code> directory.  Any changes you wish to make to the database schema should be done in their respective database folders.  The files are applied in sequential numeric steps from the current version M to the desired version N.</p> <p>For the first-ever run during the CRDB cluster initialization, the db-manager will run once to bootstrap and bring the database up to date.  To upgrade existing clusters you will need to:</p>"},{"location":"build/#if-performing-this-operation-on-the-original-cluster","title":"If performing this operation on the original cluster","text":"<ol> <li>Update the <code>desired_xyz_db_version</code> field in <code>main.jsonnet</code></li> <li>Delete the existing db-manager job in your k8s cluster</li> <li>Redeploy the newly configured db-manager with <code>tk apply -t job/&lt;xyz-schema-manager&gt;</code>. It should automatically up/down grade your database schema to your desired version.</li> </ol>"},{"location":"build/#if-performing-this-operation-on-any-other-cluster","title":"If performing this operation on any other cluster","text":"<ol> <li> <p>Create <code>workspace/$CLUSTER_CONTEXT_schema_manager</code> in this (build) directory.</p> </li> <li> <p>From this (build) working directory,     <code>cp -r ../deploy/services/tanka/examples/schema_manager/* workspace/$CLUSTER_CONTEXT_schema_manager</code>.</p> </li> <li> <p>Edit <code>workspace/$CLUSTER_CONTEXT_schema_manager/main.jsonnet</code> and replace all <code>VAR_*</code>     instances with appropriate values where applicable as explained in the above section.</p> </li> <li> <p>Run <code>tk apply workspace/$CLUSTER_CONTEXT_schema_manager</code></p> </li> </ol>"},{"location":"build/#garbage-collector-job","title":"Garbage collector job","text":"<p>Only since commit c789b2b on Aug 25, 2020 will the DSS enable automatic garbage collection of records by tracking which DSS instance is responsible for garbage collection of the record. Expired records added with a DSS deployment running code earlier than this must be manually removed.</p> <p>The Garbage collector job runs every 30 minute to delete records in RID tables that records' endtime is 30 minutes less than current time. If the event takes a long time and takes longer than 30 minutes (previous job is still running), the job will skip a run until the previous job completes.</p>"},{"location":"migration/","title":"CockroachDB and Kubernetes version migration","text":"<p>This page provides information on how to upgrade your CockroachDB and Kubernetes cluster deployed using the tools from this repository.</p>"},{"location":"migration/#cockroachdb-upgrades","title":"CockroachDB upgrades","text":"<p>CockroachDB must be upgraded on all DSS instances of the pool one after the other. The rollout of the upgrades on the whole CRDB cluster must be carefully performed in sequence to keep the majority of nodes healthy during that period and prevent downtime. For a Pooled deployment, one of the DSS Instance must take the role of the upgrade \"Leader\" and coordinate the upgrade with other \"Followers\" DSS instances. In general a CockroachDB upgrade consists of: 1. Upgrade preparation: Verify that the cluster is in a nominal state ready for upgrade. 1. Decide how the upgrade will be finalized (for major upgrades only): Like CockroachDB, we recommend disabling auto-finalization. 1. Perform the rolling upgrade: This step should be performed first by the Leader and as quickly as possible by the Followers one after the other. Note that during this period, the performance of the cluster may be impacted since, as documented by CockroachDB, \"a query that is sent to an upgraded node can be distributed only among other upgraded nodes. Data accesses that would otherwise be local may become remote, and the performance of these queries can suffer.\" 1. Roll back the upgrade (optional): Like the rolling upgrade, this step should be carefully coordinated with all DSS instances to guarantee the minimum number of healthy nodes to keep the cluster available. 1. Finish the upgrade: This step should be accomplished by the Leader.</p> <p>The following sections provide links to the CockroachDB migration documentation depending on your deployment type, which can be different by DSS instance.</p> <p>Important notes:</p> <ul> <li>Further work is required to test and evaluate the availability of the DSS during migrations.</li> <li>We recommend to review carefully the instructions provided by CockroachDB and to rehearse all migrations on a test   environment before applying them to production.</li> </ul>"},{"location":"migration/#terraform-deployment","title":"Terraform deployment","text":"<p>If a DSS instance has been deployed with terraform, first upgrade the cluster using Helm or Tanka. Then, update the variable <code>crdb_image_tag</code> in your <code>terraform.tfvars</code> to align your configuration with the new state of the cluster.</p>"},{"location":"migration/#helm-deployment","title":"Helm deployment","text":"<p>If you deployed the DSS using the Helm chart and the instructions provided in this repository, follow the instructions provided by CockroachDB <code>Cluster Upgrade with Helm</code> (See specific links below). Note that the CockroachDB documentation suggests to edit the values using <code>helm upgrade ... --set</code> commands. You will need to use the root key <code>cockroachdb</code> since the cockroachdb Helm chart is a dependency of the dss chart. For instance, setting the image tag and partition using the command line would look like this: <pre><code>helm upgrade [RELEASE_NAME] [PATH_TO_DSS_HELM] --set cockroachdb.image.tag=v24.1.3 --reuse-values\n</code></pre> <pre><code>helm upgrade [RELEASE_NAME] [PATH_TO_DSS_HELM] --set cockroachdb.statefulset.updateStrategy.rollingUpdate.partition=0 --reuse-values\n</code></pre> Alternatively, you can update <code>helm_values.yml</code> in your deployment and set the new image tag and rollout partition like this: <pre><code>cockroachdb:\n  image:\n    # ...\n    tag: # version\n  statefulset:\n    updateStrategy:\n      rollingUpdate:\n        partition: 0\n</code></pre> New values can then be applied using <code>helm upgrade [RELEASE_NAME] [PATH_TO_DSS_HELM] -f [helm_values.yml]</code>. We recommend the second approach to keep your helm values in sync with the cluster state.</p>"},{"location":"migration/#2127-to-2413","title":"21.2.7 to 24.1.3","text":"<p>CockroachDB requires to upgrade one minor version at a time, therefore the following migrations have to be performed:</p> <ol> <li>21.2.7 to 22.1: see CockroachDB Cluster upgrade for Helm.</li> <li>22.1 to 22.2: see CockroachDB Cluster upgrade for Helm.</li> <li>22.2 to 23.1: see CockroachDB Cluster upgrade for Helm.</li> <li>23.1 to 23.2: see CockroachDB Cluster upgrade for Helm.</li> <li>23.2 to 24.1.3: see CockroachDB Cluster upgrade for Helm.</li> </ol>"},{"location":"migration/#tanka-deployment","title":"Tanka deployment","text":"<p>For deployments using Tanka configuration, since no instructions are provided for Tanka specifically, we recommend to follow the manual steps documented by CockroachDB: <code>Cluster Upgrade with Manual configs</code>. (See specific links below) To apply the changes to your cluster, follow the manual steps and reflect the new values in the Leader and Followers Tanka configurations, namely the new image version (see <code>VAR_CRDB_DOCKER_IMAGE_NAME</code>) to ensure the new configuration is aligned with the cluster state.</p>"},{"location":"migration/#2127-to-2413_1","title":"21.2.7 to 24.1.3","text":"<p>CockroachDB requires to upgrade one minor version at a time, therefore the following migrations have to be performed:</p> <ol> <li>21.2.7 to 22.1: see CockroachDB Cluster upgrade with Manual configs.</li> <li>22.1 to 22.2: see CockroachDB Cluster upgrade with Manual configs.</li> <li>22.2 to 23.1: see CockroachDB Cluster upgrade with Manual configs.</li> <li>23.1 to 23.2: see CockroachDB Cluster upgrade with Manual configs.</li> <li>23.2 to 24.1.3: see CockroachDB Cluster upgrade with Manual configs.</li> </ol>"},{"location":"migration/#kubernetes-upgrades","title":"Kubernetes upgrades","text":"<p>Important notes:</p> <ul> <li>The migration plan below has been tested with the deployment of services using Helm and Tanka without Istio enabled. Note that this configuration flag has been decommissioned since #995.</li> <li>Further work is required to test and evaluate the availability of the DSS during migrations.</li> <li>It is highly recommended to rehearse such operation on a test cluster before applying them to a production environment.</li> </ul>"},{"location":"migration/#google-google-kubernetes-engine","title":"Google - Google Kubernetes Engine","text":"<p>Migrations of GKE clusters are managed using terraform.</p>"},{"location":"migration/#124-to-132","title":"1.24 to 1.32","text":"<p>For each intermediate version up to the target version (eg. if you upgrade from 1.27 to 1.30, apply thoses instructions for 1.28, 1.29, 1.30), do:</p> <p>Change your terraform.tfvars to use  by adding or updating the kubernetes_version variable: kubernetes_version =  Run terraform apply. This operation may take more than 30min. Monitor the upgrade of the nodes in the Google Cloud console. <ol> <li>Change your <code>terraform.tfvars</code> to use <code>&lt;new version&gt;</code> by adding or updating the <code>kubernetes_version</code> variable:    <pre><code>kubernetes_version = &lt;new version&gt;\n</code></pre></li> <li>Run <code>terraform apply</code>. This operation may take more than 30min.</li> <li>Monitor the upgrade of the nodes in the Google Cloud console.</li> </ol>"},{"location":"migration/#aws-elastic-kubernetes-service","title":"AWS - Elastic Kubernetes Service","text":"<p>Currently, upgrades of EKS can't be achieved reliably with terraform directly. The recommended workaround is to use the web console of AWS Elastic Kubernetes Service (EKS) to upgrade the cluster. Before proceeding, always check on the cluster page the Upgrade Insights tab which provides a report of the availability of Kubernetes resources in each version. The following sections omit this check if no resource is expected to be reported in the context of a standard deployment performed with the tools in this repository.</p>"},{"location":"migration/#125-to-132","title":"1.25 to 1.32","text":"<ol> <li>Before migrating to 1.29, upgrade aws-load-balancer-controller helm chart on your cluster using <code>terraform apply</code>. Changes introduced by PR #1167. You can verify if the operation has succeeded by running <code>helm list -n kube-system</code>. The APP VERSION shall be <code>2.12</code>.</li> </ol> <p>For each intermediate version up to the target version (eg. if you upgrade from 1.29 to 1.31, apply thoses instructions for 1.29, 1.30, 1.31), do: 1. Upgrade the cluster (control plane) using the AWS console. It should take ~15 minutes. 1. Update the Node Group in the Compute tab with Rolling Update strategy to upgrade the nodes using the AWS console.</p> <p>To finalize the upgrade, change your <code>terraform.tfvars</code> to match the target version (ie 1.32) by adding or updating the <code>kubernetes_version</code> variable:    <pre><code>kubernetes_version = 1.32\n</code></pre></p>"},{"location":"migration/#124-to-125","title":"1.24 to 1.25","text":"<ol> <li>Check for deprecated resources:<ul> <li>Click on the Upgrade Insights tab to see deprecation warnings on the cluster page.</li> <li>Evaluate errors in Deprecated APIs removed in Kubernetes v1.25. Using <code>kubectl get podsecuritypolicies</code>,   check if there is only one Pod Security Policy named <code>eks.privileged</code>. If it is the case,   according to the AWS documentation, you can proceed.</li> </ul> </li> <li>Upgrade the cluster using the AWS console. It should take ~15 minutes.</li> <li>Change your <code>terraform.tfvars</code> to use <code>1.25</code> by adding or updating the <code>kubernetes_version</code> variable:    <pre><code>kubernetes_version = 1.25\n</code></pre></li> </ol>"},{"location":"infrastructure/","title":"Infrastructure","text":""},{"location":"infrastructure/local-minikube/","title":"minikube","text":"<p>This module provide instructions to prepare a local minikube cluster.</p> <p>Minikube is going to take care of most of the work by spawning a local kubernetes cluster.</p>"},{"location":"infrastructure/local-minikube/#getting-started","title":"Getting started","text":""},{"location":"infrastructure/local-minikube/#prerequisites","title":"Prerequisites","text":"<p>Download &amp; install the following tools to your workstation:</p> <ol> <li>Install minikube (First step only).</li> <li>Install tools from Prerequisites</li> </ol>"},{"location":"infrastructure/local-minikube/#create-a-new-minikube-cluster","title":"Create a new minikube cluster","text":"<ol> <li>Run <code>minikube start -p dss-local-cluster</code> to create a new cluster.</li> <li>Run <code>minikube tunnel -p dss-local-cluster</code> and keep it running to expose LoadBalancer services.</li> </ol> <p>If needed, you can change the name of the cluster (<code>dss-local-cluster</code> in this documentation) as needed. You may also deploy multiple cluster at the same time, using different names.</p>"},{"location":"infrastructure/local-minikube/#access-to-the-cluster","title":"Access to the cluster","text":"<p>Minikube provide a UI, should you want to keep track of deployment and/or inspect the cluster. To start it, use the following command:</p> <ol> <li><code>minikube dashboard -p dss-local-cluster</code></li> </ol> <p>You can also use any other tool as needed. You can switch to the cluster's context by using the following command:</p> <ol> <li><code>kubectl config use-context dss-local-cluster</code></li> </ol>"},{"location":"infrastructure/local-minikube/#upload-or-update-local-image","title":"Upload or update local image","text":"<p>Should you want to run the local docker image that you built, run the following commands to upload / update your image</p> <ol> <li><code>minikube image -p dss-local-cluster push interuss-local/dss</code></li> </ol> <p>In the helm charts, use <code>docker.io/interuss-local/dss:latest</code> as image and be sure to set the <code>imagePullPolicy</code> to <code>Never</code>.</p>"},{"location":"infrastructure/local-minikube/#deployment-of-the-dss-services","title":"Deployment of the DSS services","text":"<p>You can now deploy the DSS services using helm charts.</p> <p>Follow the instructions in the README, especially the ones related to certificate generation and publication to the cluster. However, there are some minikube specific things to do:</p> <ul> <li>Use the <code>global.cloudProvider</code> setting with the value <code>minikube</code> and deploy the charts on the <code>dss-local-cluster</code> kubernetes context.</li> <li>To access the service, find the external IP using the <code>kubectl get services dss-dss-gateway</code> command. The port 80, without HTTPs is used.</li> </ul> <p>You may also use the tanka files to deploy the service. An example configuration is provided there.</p>"},{"location":"infrastructure/local-minikube/#clean-up","title":"Clean up","text":"<p>To delete all resources, run <code>minikube delete -p dss-local-cluster</code>.  Note that this operation can't be reverted and all data will be lost.</p>"},{"location":"infrastructure/terraform-aws-dss/","title":"terraform-aws-dss","text":"<p>This terraform module creates a Kubernetes cluster in Amazon Web Services using the Elastic Kubernetes Service (EKS) and generates the tanka files to deploy a DSS instance.</p>"},{"location":"infrastructure/terraform-aws-dss/#getting-started","title":"Getting started","text":""},{"location":"infrastructure/terraform-aws-dss/#prerequisites","title":"Prerequisites","text":"<p>Download &amp; install the following tools to your workstation:</p> <ol> <li>Install terraform.</li> <li>Install tools from Prerequisites</li> <li>Install provider specific tools:<ol> <li>Amazon Web Services</li> </ol> </li> </ol>"},{"location":"infrastructure/terraform-aws-dss/#amazon-web-services","title":"Amazon Web Services","text":"<ol> <li>Install and initialize AWS CLI.<ol> <li>Confirm successful installation with <code>aws --version</code>.</li> </ol> </li> <li>If you don't have an account, sign-up: https://aws.amazon.com/free/</li> <li>Configure terraform to connect to AWS using your account.</li> <li>We recommend to create an AWS_PROFILE using for instance <code>aws configure --profile aws-interuss-dss</code>       Before running <code>terraform</code> commands, run once in your shell: <code>export AWS_PROFILE=aws-interuss-dss</code>       Other methods are described here: https://registry.terraform.io/providers/hashicorp/aws/latest/docs#authentication-and-configuration</li> </ol>"},{"location":"infrastructure/terraform-aws-dss/#deployment-of-the-kubernetes-cluster","title":"Deployment of the Kubernetes cluster","text":"<ol> <li>Create a new folder in <code>/deploy/infrastructure/personal/</code> named, for instance, <code>terraform-aws-dss-dev</code>.</li> <li>Copy main.tf, output.tf and variables.gen.tf to the new folder.</li> <li>Copy <code>terraform.dev.example.tfvars</code> and rename to <code>terraform.tfvars</code></li> <li>Check that your new directory contains the following files:</li> <li>main.tf</li> <li>output.tf</li> <li>terraform.tfvars</li> <li>variables.gen.tf</li> <li>Set the variables in <code>terraform.tfvars</code> according to your environment. See TFVARS.gen.md for variables descriptions.</li> <li>In the new directory (ie /deploy/infrastructure/personal/terraform-aws-dss-dev), initialize terraform: <code>terraform init</code>.</li> <li>Run <code>terraform plan</code> to check that the configuration is valid. It will display the resources which will be provisioned.</li> <li>Run <code>terraform apply</code> to deploy the cluster. (This operation may take up to 15 min.)</li> <li>Configure the DNS resolution according to these instructions: Setup DNS</li> </ol>"},{"location":"infrastructure/terraform-aws-dss/#deployment-of-the-dss-services","title":"Deployment of the DSS services","text":"<p>During the successful run, the terraform job has created a new workspace for the cluster. The new workspace name corresponds to the cluster context. The cluster context can be retrieved by running <code>terraform output</code> in your infrastructure  folder (ie /deploy/infrastructure/personal/terraform-aws-dss-dev).</p> <p>It contains scripts to operate the cluster and setup the services.</p> <ol> <li>Go to the new workspace <code>/build/workspace/${cluster_context}</code>.</li> <li>Run <code>./get-credentials.sh</code> to login to kubernetes. You can now access the cluster with <code>kubectl</code>.</li> <li>If using CockroachDB:<ol> <li>Generate the certificates using <code>./make-certs.sh</code>. Follow script instructions if you are not initializing the cluster.</li> <li>Deploy the certificates using <code>./apply-certs.sh</code>.</li> </ol> </li> <li>If using Yugabyte:<ol> <li>Generate the certificates using <code>./dss-certs.sh init</code></li> <li>If joining a cluster, check <code>dss-certs.sh</code>'s help to add others CA in your pool and share your CA with others pools members.</li> <li>Deploy the certificates using <code>./dss-certs.sh apply</code>.</li> </ol> </li> <li>Go to the tanka workspace in <code>/deploy/services/tanka/workspace/${cluster_context}</code>.</li> <li>Run <code>tk apply .</code> to deploy the services to kubernetes. (This may take up to 30 min)</li> <li>Wait for services to initialize:<ul> <li>On AWS, load balancers and certificates are created by Kubernetes Operators. Therefore, it may take few minutes (~5min) to get the services up and running and generate the certificate. To track this progress, go to the following pages and check that:<ul> <li>On the EKS page, the status of the kubernetes cluster should be <code>Active</code>.</li> <li>On the EC2 page, the load balancers (1 for the gateway, 1 per cockroach nodes) are in the state <code>Active</code>.</li> </ul> </li> </ul> </li> <li>Verify that basic services are functioning by navigating to https://your-gateway-domain.com/healthy.</li> </ol>"},{"location":"infrastructure/terraform-aws-dss/#clean-up","title":"Clean up","text":"<ol> <li>Note that the following operations can't be reverted and all data will be lost.</li> <li>To delete all resources, run <code>tk delete .</code> in the workspace folder.</li> <li>Make sure that all load balancers and target groups have been deleted from the AWS region before next step.</li> <li><code>terraform destroy</code> in your infrastructure folder.</li> <li>On the EBS page, make sure to manually clean up the persistent storage. Note that the correct AWS region shall be selected.</li> </ol>"},{"location":"infrastructure/terraform-aws-dss/dns/","title":"Setup DNS","text":"<p>This page describes the options and steps required to setup DNS for a DSS deployment.</p>"},{"location":"infrastructure/terraform-aws-dss/dns/#terraform-managed","title":"Terraform managed","text":"<p>If your DNS zone is managed on the same account, it is possible to instruct terraform to create and manage it with the rest of the infrastructure.</p> <ul> <li>For Elastic Kubernetes Service (AWS), create the zone in your aws account and set the <code>aws_route53_zone_id</code>   variable with the zone id. Entries will be automatically created by terraform.   Note that the domain or the sub-domain managed by the zone must be properly delegated by the parent domain.   See instructions for subdomains delegation</li> </ul>"},{"location":"infrastructure/terraform-aws-dss/dns/#manual-setup","title":"Manual setup","text":"<p>If DNS entries are managed manually, set them up manually using the following steps:</p> <ol> <li>Retrieve IP addresses and expected hostnames: <code>terraform output</code>    Example of expected output:    <pre><code>crdb_addresses = [\n    {\n        \"address\" = \"34.65.15.23\"\n        \"expected_dns\" = \"0.interuss.example.com\"\n    },\n    {\n        \"address\" = \"34.65.146.56\"\n        \"expected_dns\" = \"1.interuss.example.com\"\n    },\n    {\n        \"address\" = \"34.65.191.145\"\n        \"expected_dns\" = \"2.interuss.example.com\"\n    },\n]\ngateway_address = {\n    \"address\" = \"35.186.236.146\"\n    \"expected_dns\" = \"dss.interuss.example.com\"  \n    \"certificate_validation_dns\" = [\n     {\n       \"managed_by_terraform\" = false\n       \"name\" = \"_6e246283563dcf58e7ed.interuss.example.com.\"\n       \"records\" = [\n          \"_6e246283563dcf58e7ed.xxxxx.acm-validations.aws.\",\n       ]\n       \"type\" = \"CNAME\"\n     },\n    ]\n}\n</code></pre></li> <li> <p>Create the following DNS A entries to point to the static ips:</p> <ul> <li><code>crdb_addresses[*].expected_dns</code></li> <li><code>gateway_address.expected_dns</code></li> </ul> </li> <li> <p>Create the entries for SSL certificate validation according to the information provided     in <code>gateway_address.certificate_validation_dns</code>.</p> </li> </ol>"},{"location":"infrastructure/terraform-google-dss/","title":"terraform-google-dss","text":"<p>This terraform module creates a Kubernetes cluster in Google Cloud Engine and generates the tanka files to deploy a DSS instance.</p>"},{"location":"infrastructure/terraform-google-dss/#getting-started","title":"Getting started","text":""},{"location":"infrastructure/terraform-google-dss/#prerequisites","title":"Prerequisites","text":"<p>Download &amp; install the following tools to your workstation:</p> <ol> <li>Install terraform.</li> <li>Install tools from Prerequisites</li> <li>Install provider specific tools:<ol> <li>Google Cloud Engine</li> </ol> </li> </ol>"},{"location":"infrastructure/terraform-google-dss/#google-cloud-engine","title":"Google Cloud Engine","text":"<ol> <li>Install and initialize Google Cloud CLI.<ol> <li>Confirm successful installation with <code>gcloud version</code>.</li> </ol> </li> <li>Check that the DSS project is correctly selected: gcloud config list project<ol> <li>Set another one if needed using: <code>gcloud config set project $GOOGLE_PROJECT_NAME</code></li> </ol> </li> <li>Enable the following API using Google Cloud CLI:<ol> <li><code>compute.googleapis.com</code></li> <li><code>container.googleapis.com</code></li> <li>If you want to manage DNS entries with terraform: <code>dns.googleapis.com</code></li> </ol> </li> <li>Install the auth plugin to connect to kubernetes: <code>gcloud components install gke-gcloud-auth-plugin</code></li> <li>Run <code>gcloud auth application-default login</code> to generate credentials to call Google Cloud Platform APIs.<ol> <li>If the result of performing the authorization indicates 404 in the browser, check whether a local dummy-oauth instance is running (using port 8085).  Stop the dummy-oauth instance if it is running.</li> </ol> </li> </ol>"},{"location":"infrastructure/terraform-google-dss/#deployment-of-the-kubernetes-cluster","title":"Deployment of the Kubernetes cluster","text":"<ol> <li>Create a new folder in <code>/deploy/infrastructure/personal/</code> named for instance <code>terraform-google-dss-dev</code>.</li> <li>Copy main.tf, output.tf and variables.gen.tf to the new folder. (Note that the modules can be added to existing projects)</li> <li>Copy <code>terraform.dev.example.tfvars</code> and rename to <code>terraform.tfvars</code></li> <li>Check that your new directory contains the following files:</li> <li>main.tf</li> <li>output.tf</li> <li>terraform.tfvars</li> <li>variables.gen.tf</li> <li>Set the variables in <code>terraform.tfvars</code> according to your environment. See TFVARS.gen.md for variables descriptions.</li> <li>In the new directory (ie /deploy/infrastructure/personal/terraform-google-dss-dev), initialize terraform: <code>terraform init</code>.</li> <li>Run <code>terraform plan</code> to check that the configuration is valid. It will display the resources which will be provisioned.</li> <li>Run <code>terraform apply</code> to deploy the cluster. (This operation may take up to 15 min.)</li> <li>Configure the DNS resolution to the public ip addresses. DNS entries can be either managed manually or handled by terraform depending on the cloud provider. See DNS for details.</li> </ol>"},{"location":"infrastructure/terraform-google-dss/#deployment-of-the-dss-services","title":"Deployment of the DSS services","text":"<p>During the successful run, the terraform job has created a new workspace for the new cluster. The new workspace name corresponds to the cluster context. The cluster context can be retrieved by running <code>terraform output</code> in your infrastructure folder (ie /deploy/infrastructure/personal/terraform-google-dss-dev).</p> <p>It contains scripts to operate the cluster and setup the services.</p> <ol> <li>Go to the new workspace <code>/build/workspace/${cluster_context}</code>.</li> <li>Run <code>./get-credentials.sh</code> to login to kubernetes. You can now access the cluster with <code>kubectl</code>.</li> <li>If using CockroachDB:<ol> <li>Generate the certificates using <code>./make-certs.sh</code>. Follow script instructions if you are not initializing the cluster.</li> <li>Deploy the certificates using <code>./apply-certs.sh</code>.</li> </ol> </li> <li>If using Yugabyte:<ol> <li>Generate the certificates using <code>./dss-certs.sh init</code></li> <li>If joining a cluster, check <code>dss-certs.sh</code>'s help to add others CA in your pool and share your CA with others pools members.</li> <li>Deploy the certificates using <code>./dss-certs.sh apply</code>.</li> </ol> </li> <li>Go to the tanka workspace in <code>/deploy/services/tanka/workspace/${cluster_context}</code>.</li> <li>Run <code>tk apply .</code> to deploy the services to kubernetes. (This may take up to 30 min)</li> <li>Wait for services to initialize:<ul> <li>On Google Cloud, the highest-latency operation is provisioning of the HTTPS certificate which generally takes 10-45 minutes. To track this progress:<ul> <li>Go to the \"Services &amp; Ingress\" left-side tab from the Kubernetes Engine page.</li> <li>Click on the https-ingress item (filter by just the cluster of interest if you have multiple clusters in your project).</li> <li>Under the \"Ingress\" section for Details, click on the link corresponding with \"Load balancer\".</li> <li>Under Frontend for Details, the Certificate column for HTTPS protocol will have an icon next to it which will change to a green checkmark when provisioning is complete.</li> <li>Click on the certificate link to see provisioning progress.</li> <li>If everything indicates OK and you still receive a cipher mismatch error message when attempting to visit /healthy, wait an additional 5 minutes before attempting to troubleshoot further.</li> </ul> </li> </ul> </li> <li>Verify that basic services are functioning by navigating to https://your-gateway-domain.com/healthy.</li> </ol>"},{"location":"infrastructure/terraform-google-dss/#clean-up","title":"Clean up","text":"<p>To delete all resources, run <code>terraform destroy</code>. Note that this operation can't be reverted and all data will be lost.</p> <p>For Google Cloud Engine, make sure to manually clean up the persistent storage: https://console.cloud.google.com/compute/disks</p>"},{"location":"infrastructure/terraform-google-dss/dns/","title":"Setup DNS","text":"<p>This page describes the options and steps required to setup DNS for a DSS deployment.</p>"},{"location":"infrastructure/terraform-google-dss/dns/#terraform-managed","title":"Terraform managed","text":"<p>If your DNS zone is managed on the same account, it is possible to instruct terraform to create and manage it with the rest of the infrastructure.</p> <ul> <li>For Google Cloud Engine, configure the zone in your google account and set the <code>google_dns_managed_zone_name</code>   variable the zone name. Zones can be listed by running <code>gcloud dns managed-zones list</code>. Entries will be   automatically created by terraform.</li> </ul>"},{"location":"infrastructure/terraform-google-dss/dns/#manual-setup","title":"Manual setup","text":"<p>If DNS entries are managed manually, set them up manually using the following steps:</p> <ol> <li> <p>Retrieve IP addresses and expected hostnames: <code>terraform output</code>    Example of expected output:    ```    crdb_addresses = [        {            \"address\" = \"34.65.15.23\"            \"expected_dns\" = \"0.interuss.example.com\"        },        {            \"address\" = \"34.65.146.56\"            \"expected_dns\" = \"1.interuss.example.com\"        },        {            \"address\" = \"34.65.191.145\"            \"expected_dns\" = \"2.interuss.example.com\"        },    ]    gateway_address = {        \"address\" = \"35.186.236.146\"        \"expected_dns\" = \"dss.interuss.example.com\"    }</p> </li> <li> <p>Create the related DNS A entries to point to the static ips.</p> </li> </ol>"},{"location":"operations/","title":"Operations","text":"<p>This folder contains the instructions and related material used to operate a DSS. It is responsible to provide diagnostic capabilities and utilities to operate the DSS instance, such as certificates management.</p> <p>Currently, the operations scripts are located inside build and if using the infrastructure layer, helpers are generated in the workspace directory by terraform after deployment.</p> <p>As a complete example, the configuration files used by the CI job of the infrastructure and services layers are located in ci.</p>"},{"location":"operations/#pooling-procedure","title":"Pooling procedure","text":""},{"location":"operations/#creating-a-new-pool","title":"Creating a new pool","text":"<p>See Creating a new pool</p>"},{"location":"operations/#establishing-a-pool-with-first-instance","title":"Establishing a pool with first instance","text":"<p>See Establishing a pool with first instance</p>"},{"location":"operations/#joining-an-existing-pool-with-new-instance","title":"Joining an existing pool with new instance","text":"<p>See Joining an existing pool with new instance</p>"},{"location":"operations/#leaving-a-pool","title":"Leaving a pool","text":"<p>See Leaving a pool</p>"},{"location":"operations/#monitoring","title":"Monitoring","text":"<p>See Monitoring</p>"},{"location":"operations/#troubleshooting","title":"Troubleshooting","text":"<p>See Troubleshooting</p>"},{"location":"operations/certificates-management/","title":"Certificates management","text":""},{"location":"operations/certificates-management/#introduction","title":"Introduction","text":"<p>The <code>dss-certs.py</code> helps you manage the set of certificates used for your DSS deployment.</p> <p>Should this DSS beeing part of a pool, the script also provide some helpers to manage the set of CA certificates in the pool.</p> <p>To run the script, just run <code>./dss-certs.py</code>. The python script don't require any dependencies, just a recent version of python 3.</p>"},{"location":"operations/certificates-management/#quick-start-guide","title":"Quick start guide","text":""},{"location":"operations/certificates-management/#single-dss-instance-in-minikube","title":"Single DSS instance in minikube`","text":"<ul> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default init</code></li> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default apply</code></li> </ul>"},{"location":"operations/certificates-management/#pool-of-3-dss-instances-in-minikube-in-namespace-default-ns2-and-ns3","title":"Pool of 3 DSS instances in minikube, in namespace <code>default</code>, <code>ns2</code> and <code>ns3</code>","text":"<ul> <li>Creation of the 3 DSS instances certificates</li> <li><code>./dss-certs.py --name dss-instance-1 --cluster-context dss-local-cluster --namespace default init</code></li> <li><code>./dss-certs.py --name dss-instance-2 --cluster-context dss-local-cluster --namespace ns2 init</code></li> <li><code>./dss-certs.py --name dss-instance-1 --cluster-context dss-local-cluster --namespace ns3 init</code></li> <li>Copy instance 2 and 3 CA certificates to the instance 1</li> <li><code>./dss-certs.py --name dss-instance-2 --cluster-context dss-local-cluster --namespace ns2 get-ca | ./dss-certs.py --name dss-instance-1 --cluster-context dss-local-cluster --namespace default add-pool-ca</code></li> <li><code>./dss-certs.py --name dss-instance-3 --cluster-context dss-local-cluster --namespace ns3 get-ca | ./dss-certs.py --name dss-instance-1 --cluster-context dss-local-cluster --namespace default add-pool-ca</code></li> <li>Reuse instance compiled 1 CA and copy it to instance 2 and 3.</li> <li><code>./dss-certs.py --name dss-instance-1 --cluster-context dss-local-cluster --namespace default get-pool-ca | ./dss-certs.py --name dss-instance-2 --cluster-context dss-local-cluster --namespace ns2 add-pool-ca</code></li> <li><code>./dss-certs.py --name dss-instance-1 --cluster-context dss-local-cluster --namespace default get-pool-ca | ./dss-certs.py --name dss-instance-3 --cluster-context dss-local-cluster --namespace ns3 add-pool-ca</code></li> <li>Application of certificates in respective clusters</li> <li><code>./dss-certs.py --name dss-instance-1 --cluster-context dss-local-cluster --namespace default apply</code></li> <li><code>./dss-certs.py --name dss-instance-2 --cluster-context dss-local-cluster --namespace ns2 apply</code></li> <li><code>./dss-certs.py --name dss-instance-3 --cluster-context dss-local-cluster --namespace ns3 apply</code></li> </ul>"},{"location":"operations/certificates-management/#operations","title":"Operations","text":""},{"location":"operations/certificates-management/#common-parameters","title":"Common parameters","text":""},{"location":"operations/certificates-management/#-name","title":"<code>--name</code>","text":"<p>The name of your DSS instance, that should identify it in a unique way. Used as main identifier for the set of certificates and in certificates.</p> <p>Example: <code>dss-west-1</code></p>"},{"location":"operations/certificates-management/#-organization","title":"<code>--organization</code>","text":"<p>The name of the organization managing the DSS Instance. Used in certificates generation. The combination of (name, organization) shall be unique in a cluster.</p> <p>Example: <code>Interuss</code></p>"},{"location":"operations/certificates-management/#-cluster-context","title":"<code>--cluster-context</code>","text":"<p>The kubernetes context the script should use.</p> <p>Example: <code>dss-local-cluster</code></p>"},{"location":"operations/certificates-management/#-namespace","title":"<code>--namespace</code>","text":"<p>The kubernetes namespace to use.</p> <p>Example: <code>default</code></p>"},{"location":"operations/certificates-management/#-nodes-count","title":"<code>--nodes-count</code>","text":"<p>The number of yugabyte nodes of your DSS instance. Default to <code>3</code>.</p>"},{"location":"operations/certificates-management/#init","title":"<code>init</code>","text":"<p>Initializes the certificates for a new DSS instance including a CA, a client certificate and a certificate for each yugabyte node.</p>"},{"location":"operations/certificates-management/#apply","title":"<code>apply</code>","text":"<p>Apply the current set of certificates to the kubernetes cluster. Shall be ran after each modification of the certificates, like addition / removal of CA in the pool, new <code>nodes-count</code> parameter.</p>"},{"location":"operations/certificates-management/#regenerate-nodes","title":"<code>regenerate-nodes</code>","text":"<p>Generate missing nodes certificates. Useful if you want to add new nodes in your DSS Instance. Don't forget to set the <code>nodes-count</code> parameters.</p>"},{"location":"operations/certificates-management/#add-pool-ca","title":"<code>add-pool-ca</code>","text":"<p>Add a CA certificate(s) of another(s) DSS Instance to the set of trusted certificates. Existing certificates are not added again.</p> <p>You can set the file with certificate(s) with <code>--ca-file</code> or use stdin.</p> <p>Don't forget to use the <code>apply</code> command to update certificate on your kubernetes cluster.</p> <p>Examples:</p> <ul> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default add-pool-ca &lt; /tmp/new-dss-ca</code></li> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default --ca-file /tmp/new-dss-ca add-pool-ca</code></li> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default get-pool-ca | ./dss-certs.py --name test2 --cluster-context dss-local-cluster --namespace namespace2 add-pool-ca</code></li> </ul>"},{"location":"operations/certificates-management/#remove-pool-ca","title":"<code>remove-pool-ca</code>","text":"<p>Remove CA certificate(s) of DSS Instance(s) from the set of trusted certificates. Unknown certificates are not removed again.</p> <p>You can set the file with certificate(s) with <code>--ca-file</code>, use stdin or use <code>--ca-serial</code> to specify the serial / name of the certificate you want to remove.</p> <p>Don't forget to use the <code>apply</code> command to update certificate on your kubernetes cluster.</p> <p>Example:</p> <ul> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default remove-pool-ca &lt; /tmp/old-dss-ca</code></li> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default --ca-file /tmp/old-dss-ca remove-pool-ca</code></li> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default remove-pool-ca --ca-serial=\"SN=830ECFB0, O=generic-dss-organization, CN=CA.test\"</code></li> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default remove-pool-ca --ca-serial=\"830ECFB0</code></li> <li><code>./dss-certs.py --name test --cluster-context dss-local-cluster --namespace default remove-pool-ca --ca-serial=\"46548B7CC9699A7CFA54FF8FA85A619E830ECFB0</code></li> </ul>"},{"location":"operations/certificates-management/#list-pool-ca","title":"<code>list-pool-ca</code>","text":"<p>List the set of accepted CA certificates.</p> <p>Also display a 'hash' of CA serial, that you may use to compare other DSS Instances list of CA certificates easily.</p>"},{"location":"operations/certificates-management/#get-pool-ca","title":"<code>get-pool-ca</code>","text":"<p>Return all CA certificate in the current pool.</p> <p>Can be used for debugging or to synchronize the set of CA certificates in a pool with others USS.</p>"},{"location":"operations/certificates-management/#get-ca","title":"<code>get-ca</code>","text":"<p>Return your own CA certificate .</p> <p>Display the compiled CA certificate. Can be used for debugging or to synchronize the set of CA certificates in a pool with others USS.</p>"},{"location":"operations/certificates-management/#destroy","title":"<code>destroy</code>","text":"<p>Destroy a certificate set. Be careful, there are no way to undo the command.</p>"},{"location":"operations/monitoring/","title":"Monitoring","text":""},{"location":"operations/monitoring/#prerequisites","title":"Prerequisites","text":"<p>Some of the tools from the manual deployment documentation are required to interact with monitoring services.</p>"},{"location":"operations/monitoring/#grafana-prometheus","title":"Grafana / Prometheus","text":"<p>Note: this monitoring stack is only currently brought up when deploying services with tanka.</p> <p>By default, an instance of Grafana and Prometheus are deployed along with the core DSS services; this combination allows you to view (Grafana) CRDB metrics (collected by Prometheus).  To view Grafana, first ensure that the appropriate cluster context is selected (<code>kubectl config current-context</code>).  Then, run the following command:</p> <p><code>shell script kubectl get pod | grep grafana | awk '{print $1}' | xargs -I {} kubectl port-forward {} 3000</code></p> <p>While that command is running, open a browser and navigate to http://localhost:3000.  The default username is <code>admin</code> with a default password of <code>admin</code>.  Click the magnifying glass on the left side to select a dashboard to view.</p>"},{"location":"operations/monitoring/#prometheus-federation-multi-cluster-monitoring","title":"Prometheus Federation (Multi Cluster Monitoring)","text":"<p>The DSS can use Prometheus to gather metrics from the binaries deployed with this project, by scraping formatted metrics from an application's endpoint. Prometheus Federation enables you to easily monitor multiple clusters of the DSS that you operate, unifying all the metrics into a single Prometheus instance where you can build Grafana Dashboards for. Enabling Prometheus Federation is optional. To enable you need to do 2 things: 1. Externally expose the Prometheus service of the DSS clusters. 2. Deploy a \"Global Prometheus\" instance to unify metrics.</p>"},{"location":"operations/monitoring/#externally-exposing-prometheus","title":"Externally Exposing Prometheus","text":"<p>You will need to change the values in the <code>prometheus</code> fields in your metadata tuples: 1. <code>expose_external</code> set to <code>true</code> 2. [Optional] Supply a static external IP Address to <code>IP</code> 3. [Highly Recommended] Supply whitelists of IP Blocks in CIDR form, leaving an empty list mean everyone can publicly access your metrics. 4. Then Run <code>tk apply ...</code> to deploy the changes on your DSS clusters.</p>"},{"location":"operations/monitoring/#deploy-global-prometheus-instance","title":"Deploy \"Global Prometheus\" instance","text":"<ol> <li>Follow guide to deploy Prometheus https://prometheus.io/docs/introduction/first_steps/</li> <li>The scrape rules for this global instance will scrape other prometheus <code>/federate</code> endpoint and rather simple, please look at the example configuration.</li> </ol>"},{"location":"operations/monitoring/#health-checks","title":"Health checks","text":"<p>This section describes various monitoring activities a USS may perform to verify various characteristics of their DSS instance and its pool.  In general, they rely on a DSS operator's monitoring infrastructure querying particular endpoints, evaluating the results of those queries, and producing alerts under certain conditions.  Not all checks listed below are fully implemented in the current InterUSS implementation.</p> <p>One or more procedures below could be implemented into a single, more-accessible endpoint in monitoring middleware.</p>"},{"location":"operations/monitoring/#healthy-check","title":"/healthy check","text":""},{"location":"operations/monitoring/#summary","title":"Summary","text":"<p>Checks whether a DSS instance is responsive to HTTPS requests.</p>"},{"location":"operations/monitoring/#procedure","title":"Procedure","text":"<p>For each expected DSS instance in the pool, query <code>/healthy</code></p>"},{"location":"operations/monitoring/#alert-criteria","title":"Alert criteria","text":"<ul> <li>Any query failed or returned a code other than 200</li> </ul>"},{"location":"operations/monitoring/#normal-usage-metrics","title":"Normal usage metrics","text":""},{"location":"operations/monitoring/#summary_1","title":"Summary","text":"<p>Checks whether normal calls to USS's DSS instance generally succeed.</p>"},{"location":"operations/monitoring/#procedure_1","title":"Procedure","text":"<p>USS notifies its monitoring system whenever a normal ASTM-API call to its DSS instance fails due to an error indicating a failed service like timeout, 5xx, 405, 408, 418, 451, and possibly others.</p>"},{"location":"operations/monitoring/#alert-criteria_1","title":"Alert criteria","text":"<ul> <li>Number of failures per time period crosses threshold</li> </ul>"},{"location":"operations/monitoring/#dar-identity-check","title":"DAR identity check","text":""},{"location":"operations/monitoring/#summary_2","title":"Summary","text":"<p>Checks whether a set of DSS instances indicate that they are using the same DSS Airspace Representation (DAR).</p>"},{"location":"operations/monitoring/#procedure-option-1","title":"Procedure, Option 1","text":"<p>For each expected DSS instance in the pool, query <code>/aux/v1/pool</code> and collect <code>dar_id</code></p>"},{"location":"operations/monitoring/#alert-criteria-option-1","title":"Alert criteria, Option 1","text":"<ul> <li>Any query failed</li> <li>Any collected <code>dar_id</code> value is different from any other collected <code>dar_id</code> value</li> </ul>"},{"location":"operations/monitoring/#procedure-option-2","title":"Procedure, Option 2","text":"<p>Prior to ongoing operations, exchange the expected DAR ID for the environment among all DSS operators.</p> <p>On an ongoing basis, query <code>/aux/v1/pool</code> on DSS operator's DSS instance and collect <code>dar_id</code></p>"},{"location":"operations/monitoring/#alert-criteria-option-2","title":"Alert criteria, Option 2","text":"<ul> <li>Query failed</li> <li>Collected <code>dar_id</code> differs from expected DAR ID for the environment</li> </ul>"},{"location":"operations/monitoring/#per-uss-heartbeat-check","title":"Per-USS heartbeat check","text":"<p>Note: the implementation of this functionality is not yet complete.</p>"},{"location":"operations/monitoring/#summary_3","title":"Summary","text":"<p>Checks whether all DSS instance operators have recently verified their ability to synchronize data to another DSS instance operator.</p>"},{"location":"operations/monitoring/#procedure_2","title":"Procedure","text":"<p>DSS instance operators agree to all configure their monitoring and alerting systems to execute this procedure, with an agreed-upon maximum time interval:</p> <p>Assert a new heartbeat for the DSS operator's DSS instance via <code>PUT /aux/v1/pool/dss_instances/heartbeat</code> which returns the list of <code>dss_instances</code> including each one's <code>most_recent_heartbeat</code></p>"},{"location":"operations/monitoring/#alert-criteria_2","title":"Alert criteria","text":"<ul> <li><code>PUT</code> query fails</li> <li>Any expected DSS instance in the pool does not have an entry in <code>dss_instances</code></li> <li>The current time is past any DSS instance's <code>next_heartbeat_expected_before</code></li> <li>The difference between <code>next_heartbeat_expected_before</code> and <code>timestamp</code> is larger than the agreed-upon maximum time interval for any DSS instance</li> </ul>"},{"location":"operations/monitoring/#nonce-exchange-check","title":"Nonce exchange check","text":"<p>Note: none of this functionality has been implemented yet.</p>"},{"location":"operations/monitoring/#summary_4","title":"Summary","text":"<p>Definitively checks whether pool data written into one DSS instance can be read from another DSS instance.</p>"},{"location":"operations/monitoring/#implementation","title":"Implementation","text":"<p>This check would involve establishing the ability to read and write (client USS ID, DSS instance writer ID, nonce value) triplets in a database table describing pool information.</p>"},{"location":"operations/monitoring/#procedure_3","title":"Procedure","text":"<ol> <li>For each expected DSS instance in the pool, write a nonce value</li> <li>For each expected DSS instance in the pool, read all (DSS instance writer ID, nonce value) pairs written by the DSS instance operator's client USS ID</li> </ol>"},{"location":"operations/monitoring/#alert-criteria_3","title":"Alert criteria","text":"<ul> <li>Any query failed</li> <li>The nonce value written to DSS instance i does not match the nonce value that DSS instance j reports was written to DSS instance i by the DSS instance operator</li> </ul>"},{"location":"operations/monitoring/#dss-entity-injection-check","title":"DSS entity injection check","text":""},{"location":"operations/monitoring/#summary_5","title":"Summary","text":"<p>Actual DSS entities (subscriptions, operational intents) are manipulated in a geographically-isolated test area.</p>"},{"location":"operations/monitoring/#procedure_4","title":"Procedure","text":"<p>Run uss_qualifier with a suitable configuration.</p> <p>The suitable configuration would cause DSS entities to be created, read, updated, and deleted within an isolated geographical test area, likely via a subset of the dss all_tests automated test suite with uss_qualifier possessing USS-level credentials.</p>"},{"location":"operations/monitoring/#alert-criteria_4","title":"Alert criteria","text":"<ul> <li>Tested requirements artifact does not indicate Pass</li> </ul>"},{"location":"operations/monitoring/#database-metrics-check","title":"Database metrics check","text":""},{"location":"operations/monitoring/#summary_6","title":"Summary","text":"<p>Certain metrics exposed by the underlying database software are monitored.</p>"},{"location":"operations/monitoring/#procedure_5","title":"Procedure","text":"<p>Each USS queries metrics of underlying database software (CRDB, YugabyteDB) using their database node(s).</p>"},{"location":"operations/monitoring/#alert-criteria_5","title":"Alert criteria","text":"<ul> <li>Any Raft quorum unavailability</li> <li>Resource usage within threshold of ceiling for resource (e.g., 90% of storage/memory/CPU on node in use)</li> <li>Any SQL failures</li> </ul>"},{"location":"operations/monitoring/#failure-detection-capability","title":"Failure detection capability","text":"<p>This section summarizes the preceding health checks and their ability to detect failures.</p>"},{"location":"operations/monitoring/#potential-failures","title":"Potential failures","text":"<p>This list of failures and potential causes is not exhaustive in either respect.</p> <ol> <li>DSS instance is not accepting incoming HTTPS requests<ol> <li>Deployment not complete</li> <li>HTTP(S) ingress/certificates/routing/etc not configured correctly</li> <li>DNS not configured correctly</li> </ol> </li> <li>Database components of DSS instance are non-functional<ol> <li>Database container not deployed correctly</li> <li>Database functionality failing</li> <li>Database software not behaving as expected</li> <li>Connectivity (e.g., username/password) between core-service and database not configured correctly</li> <li>System-range quorum of database nodes not met</li> <li>Trusted certificates for the pool not exchanged or configured correctly</li> </ol> </li> <li>USS initializes a stand-alone DSS instance or connects to a different pool rather than joining the intended pool<ol> <li>Database initialization parameter not set properly during deployment + nodes to join omitted</li> <li>Nodes to join + trusted certificates specified incorrectly</li> </ol> </li> <li>USS shared the wrong base URL for their DSS instance with other pool participants<ol> <li>I.e., USS deployed and uses fully-functional DSS instance at https://dss_q.uss.example.com connected to the DSS pool for environment Q, but indicates to other USSs that the DSS instance for environment Q is located at https://dss_r.uss.example.com (another fully-functional DSS instance connected to a different pool)</li> <li>Note: the likelihood of this failure could be reduced to negligible if DSS base URLs were included in #1140</li> </ol> </li> <li>DSS instance can interact with the database, but cannot read from/write to any tables<ol> <li>DSS instance operator executed InterUSS-unsupported manual commands directly to the database to change the access rights of database users used by DSS instances</li> </ol> </li> <li>DSS instance can read from and write to pool table, but cannot read from/write to SCD/RID tables<ol> <li>DSS instance operator executed InterUSS-unsupported manual commands directly to the database to change the access rights of database users used by DSS instances on a per-table basis</li> <li>SCD/RID tables not initialized</li> <li>SCD/RID tables corrupt or not at appropriate schema version</li> </ol> </li> <li>The DSS instance connected to the pool is not used by the USS in the pool's environment<ol> <li>USS specified the wrong DSS base URL in the rest of their system in the pool environment<ol> <li>E.g., DSS instance at https://dss_x.uss.example.com is fully functional, connects to the DSS pool for environment X and is the base URL USS shares with other USSs, but the USS specifies https://dss_y.uss.example.com as the DSS instance for the rest of their system to use in environment X</li> </ol> </li> <li>USS did not configure their system to use features (e.g., ASTM F3548-21 strategic coordination) requiring a DSS in the test of their system in the pool environment</li> </ol> </li> <li>DSS instance is working, but another part of the owning USS's system has failed<ol> <li>USS deploys their DSS instance differently than/separately from the rest of their system, and the rest-of-system deployment failed while the DSS instance deployment is unaffected</li> <li>A component in the rest of the USS's system failed</li> </ol> </li> <li>Database software indicates success to the core-service client, but does not correctly synchronize data to other DSS instances<ol> <li>There is a critical bug in the database software (this would seem to be a product problem rather than a configuration problem)</li> </ol> </li> <li>Aux API works but SCD/RID API does not work or is disabled<ol> <li>DSS instance configuration does not enable SCD/RID APIs as needed</li> <li>SCD/RID endpoint routing does not work (though other routing does work)</li> </ol> </li> <li>Database nodes are unavailable such that quorum is not met for certain ranges<ol> <li>Database node container(s) run out of disk space</li> <li>Database node container(s) are shut down due to resource shortage</li> <li>System maintenance conducted improperly (for instance, multiple USSs bring down nodes contributing to the same range for maintenance simultaneously)</li> </ol> </li> <li>Everything is working properly, but the system lacks the capacity to handle the volume of traffic</li> </ol>"},{"location":"operations/monitoring/#check-detection-capabilities","title":"Check detection capabilities","text":"Check (+readiness) Failure 1 2 3 4 5 6 7 8 9 10 11 12 \ud83d\ude80 /healthy \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \ud83d\udee0\ufe0f Normal usage metrics \u2705 \u2705 \u274c \u274c \u2705 \u2705 \u274c \ud83d\udd36 \u274c \u2705 \ud83d\udd36 \ud83d\udd36 \u2705 DAR identity \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \ud83d\udd36 \ud83d\udd36\u2193\u2193 \ud83d\udea7 Per-USS heartbeat \u2705 \u2705 \u2705 \u274c \u2705 \u274c \ud83d\udd36 \ud83d\udd36 \u2705 \u274c \ud83d\udd36 \ud83d\udd36\u2193 \ud83d\udea7 Nonce exchange \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c \ud83d\udd36 \ud83d\udd36\u2193 \ud83d\ude80 DSS entity injection \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u2705 \ud83d\udd36 \ud83d\udd36 \ud83d\udee0\ufe0f Database metrics \u274c \ud83d\udd36 \u274c \u274c \u274c \u274c \u274c \u274c \ud83d\udd36 \u274c \u2705 \u2705"},{"location":"operations/monitoring/#legend","title":"Legend","text":"Readiness \ud83d\ude80Released Failure detection \u2705Detects failure \u2705Complete (not yet released) \ud83d\udd36May detect failure \ud83d\udea7Not complete \ud83d\udd36\u2193Might rarely detect failure \ud83d\udee0\ufe0fRequires user involvement \ud83d\udd36\u2193\u2193Might very rarely detect failure \u274cDoes not detect failure"},{"location":"operations/pooling-crdb/","title":"DSS Pooling","text":"<p>[!WARNING] This document is about pooling with CockroachDB. Yugabyte documentation is there.</p>"},{"location":"operations/pooling-crdb/#introduction","title":"Introduction","text":"<p>The DSS is designed to be deployed in a federated manner where multiple organizations each host a DSS instance, and all of those instances interoperate. Specifically, if a change is made on one DSS instance, that change may be read from a different DSS instance.  A set of interoperable DSS instances is called a \"pool\", and the purpose of this document is to describe how to form and maintain a DSS pool.</p> <p>It is expected that there will be exactly one production DSS pool for any given DSS region, and that a DSS region will generally match aviation jurisdictional boundaries (usually national boundaries).  A given DSS region (e.g., Switzerland) will likely have one pool for production operations, and an additional pool for partner qualification and testing (per, e.g., F3411-19 A2.6.2).</p>"},{"location":"operations/pooling-crdb/#terminology-notes","title":"Terminology notes","text":"<p>CockroachDB (CRDB) establishes a distributed data store called a \"cluster\". This cluster stores the DSS Airspace Representation (DAR) in multiple SQL databases within that cluster.  This cluster is composed of many CRDB nodes, potentially hosted by multiple organizations.</p> <p>Kubernetes manages a set of services in a \"cluster\".  This is an entirely different thing from the CRDB data store, and this type of cluster is what the deployment instructions refer to.  A Kubernetes cluster contains one or more node pools: collections of machines available to run jobs.  This node pool is an entirely different thing from a DSS pool.</p>"},{"location":"operations/pooling-crdb/#objective","title":"Objective","text":"<p>A pool of InterUSS-compatible DSS instances is established when all of the following requirements are met:</p> <ol> <li>Each CockroachDB node is addressable by every other CockroachDB node</li> <li>Each CockroachDB node is discoverable</li> <li>Each CockroachDB node accepts the certificates of every other node</li> <li>The CockroachDB cluster is initialized</li> </ol> <p>The procedures in this document are intended to achieve all the objectives listed above, but these procedures are not the only ways to achieve the objectives.</p>"},{"location":"operations/pooling-crdb/#each-cockroachdb-node-is-addressable-by-every-other-cockroachdb-node","title":"\"Each CockroachDB node is addressable by every other CockroachDB node\"","text":"<p>Every CRDB node must have its own externally-accessible hostname (e.g., 1.db.dss-prod.example.com), or its own hostname:port combination (e.g., db.dss-prod.example.com:26258).  The port on which CRDB communicates must be open (default CRDB port is 26257).</p> <p>This requirement may be verified by conducting a standard TLS diagnostic (like this one) on the hostname:port for each CRDB node (e.g., 0.db.dss.example.com:26257).  The \"Trust\" characteristic will not pass because the certificate is issued by \"Cockroach CA\" which is not a generally-trusted root CA, but we explicitly enable trust by manually exchanging the trusted CA public keys in ca.crt (see \"Each CockroachDB node accepts the certificates of every other node\" below).  However, all other checks should generally pass.</p>"},{"location":"operations/pooling-crdb/#each-cockroachdb-node-is-discoverable","title":"\"Each CockroachDB node is discoverable\"","text":"<p>When a CRDB node is brought online, it must know how to connect to the existing network of nodes.  This is accomplished by providing an explicit list of nodes to contact.  Each node contacted will provide a list of nodes it is connected to in the network (\"gossip\"), so not every node must be present in the explicit list.  The explicit list should contain, at a minimum, all nodes within the DSS instance, and at least one node from each other DSS instance.  Standard practice should generally include two nodes from each other DSS instance in case one node is down for maintenance.</p>"},{"location":"operations/pooling-crdb/#each-cockroachdb-node-accepts-the-certificates-of-every-other-node","title":"\"Each CockroachDB node accepts the certificates of every other node\"","text":"<p>CockroachDB uses TLS to secure connections, and TLS includes a mechanism to ensure the identity of the server being contacted.  This mechanism requires a trusted root Certificate Authority (CA) to sign a certificate containing the public key of a particular server, so a client connecting to that server can verify that the certificate (containing the public key) is endorsed by the root CA as being genuine.  CRDB certificates require a claim that standard web CAs will not sign, so instead each USS acts as their own root CA.  When USS 1 is presented with certificates signed by USS 2's CA, USS 1 must know that it can trust that certificate.  We accomplish this by exchanging all USSs' CA public keys out-of-band in ca.crt, and specifying that certificates signed by any of the public keys in ca.crt should be accepted when considering the validity of certificates presented to establish a TLS connection between nodes.</p> <p>The private CA key make-certs.py generates is stored in ca_key_dir, and its corresponding public key is stored in ca_certs_dir (ca.crt).  The private CA key is used to generate all node certificates and client certificates.  Once a pool is established, a USS avoids regenerating this CA keypair, and use the existing ones by default. This default behavior can be modified by setting --overwrite-ca-cert flag to true. If a USS generates a new CA keypair, the new public key must be added to the pool's combined ca.crt, and all USSs in the pool must adopt the new combined ca.crt before any nodes using certificates generated by the new CA private key will be accepted by the pool.</p>"},{"location":"operations/pooling-crdb/#the-cockroachdb-cluster-is-initialized","title":"\"The CockroachDB cluster is initialized\"","text":"<p>A CRDB cluster of databases is like the Ship of Theseus: it is composed of many nodes which may all be replaced, one by one, so that a given CRDB cluster eventually contains none of its original nodes.  Unlike the Ship of Theseus, however, a cluster is clearly identified by its cluster ID (e.g., b2537de3-166f-42c4-aae1-742e094b8349) -- if the cluster ID is the same, it is the same cluster (and vice versa).  Once for the entire lifetime of the CRDB cluster, it is created and given its cluster ID with the <code>cockroach init</code> command.  If this command is run on a node that is part of a cluster that has already been initialized, it will fail.  The Kubernetes configuration in this repository uses the <code>VAR_SHOULD_INIT</code> parameter to control whether this command is executed during deployment.</p>"},{"location":"operations/pooling-crdb/#additional-requirements","title":"Additional requirements","text":"<p>These requirements must be met by every DSS instance joining an InterUSS-compatible pool.  The deployment instructions produce a system that complies with all these requirements, so this section may be ignored if following those instructions.</p> <ul> <li>All CockroachDB nodes must be run in secure mode, by supplying the   <code>--certs-dir</code> and <code>--ca-key</code> flags.</li> <li>Do not specify <code>--insecure</code></li> <li>The ordering of the <code>--locality</code> flag keys must be the same across all   CockroachDB nodes in the cluster.</li> <li>All DSS instances in the same cluster must point their ntpd at the same NTP   Servers.   CockroachDB recommends   using   Google's Public NTP when running in a   multi-cloud environment.</li> </ul>"},{"location":"operations/pooling-crdb/#creating-a-new-pool","title":"Creating a new pool","text":"<p>Although all DSS instances are equal peers, one DSS instance must be chosen to create the pool initially.  After the pool is established, one additional DSS instance can join it.  After that joining process is complete, it can be repeated any number of times to add additional DSS instances, though 7 is the maximum recommended number of DSS instances for performance reasons.  The following diagram illustrates the pooling process for the first two instances:</p> <p></p> <p>The nth instance joins in almost the same way as the second instance; the diagram below illustrates action dependencies between the existing and joining DSS instances to allow the new instance to join the existing pool:</p> <p></p>"},{"location":"operations/pooling-crdb/#establishing-a-pool-with-first-instance","title":"Establishing a pool with first instance","text":"<p>The USS owning the first DSS instance should follow the deployment instructions.  They are not joining any existing cluster, and specifically <code>VAR_SHOULD_INIT</code> should be set <code>true</code> to initialize the CRDB cluster.  Upon deployment completion, the following should be run against the DSS instance to verify functionality:</p> <ul> <li>The prober test</li> <li>The USS qualifier, using the DSS Probing configuration</li> </ul>"},{"location":"operations/pooling-crdb/#joining-an-existing-pool-with-new-instance","title":"Joining an existing pool with new instance","text":"<p>A USS wishing to join an existing pool (of perhaps just one instance following the prior section) should follow the deployment instructions.  They will be joining an existing cluster, and they will need to request the ca.crt that the pool is currently using (any one member of the pool may provide it). The joining USS will also need a list of node addresses to which they should connect; ideally this will include at least 2 nodes from each existing DSS instance.</p> <p>As soon as the joining USS has created a new, combined ca.crt by joining the existing ca.crt provided by the pool, the joining USS must provide that new ca.crt to every existing DSS instance in the pool, and all nodes of all instances must adopt the new, combined ca.crt before the joining USS can bring their DSS instance online (with <code>tk apply</code>).</p> <p>Once all participants in the existing pool have confirmed that the new ca.crt has been adopted by all of their nodes, the joining USS brings their system online with <code>tk apply</code>.  The pool should then be re-verified for functionality by running the prober test on each DSS instance, and the interoperability test scenario on the full pool (including the newly-added instance).</p> <p>Finally, the joining USS should provide its node addresses to all other participants in the pool, and each other participant should add those addresses to the list of CRDB nodes their CRDB nodes will attempt to contact upon restart.</p>"},{"location":"operations/pooling-crdb/#leaving-a-pool","title":"Leaving a pool","text":"<p>In an event that requires removing CockroachDB nodes we need to properly and safely decommission to reduce risks of outages.</p> <p>It is never a good idea to take down more than half the number of nodes available in your cluster as doing so would break quorum. If you need to take down that many nodes please do it in smaller steps.</p> <p>Note: If you are removing a specific node in a Statefulset, please know that Kubernetes does not support removal of specific node; it automatically re-creates the node if you delete it with <code>kubectl delete pod</code>.  You will need to scale down the Statefulset and that removes the last node first (ex: <code>cockroachdb-n</code> where <code>n</code> is the <code>size of statefulset - 1</code>, <code>n</code> starts at 0)</p> <ol> <li>Check if all nodes are healthy and there are no    under-replicated/unavailable ranges:</li> </ol> <p><code>kubectl exec -it cockroachdb-0 -- cockroach node status --ranges --certs-dir=cockroach-certs/</code></p> <pre><code>1. If there are under-replicated ranges changes are it is because of a node\n   failure. If all nodes are healthy then it should auto recover.\n\n1. If there are unhealthy nodes please investigate and fix them so that the\n   ranges can return to a healthy state\n</code></pre> <ol> <li>Identify the node id we intend to decommission from the previous commands    then decommission them. The following command assumes that <code>cockroachdb-0</code> is    not targeted for decommission otherwise select a different instance to    connect to:</li> </ol> <p><code>kubectl exec -it cockroachdb-0 -- cockroach node decommission &lt;node id 1&gt; [&lt;node id 2&gt; ...] --certs-dir=cockroach-certs/</code></p> <ol> <li> <p>If the command executes successfully all targeted nodes should not host any    ranges. Repeat step one to verify</p> <p>a. In the event of a hung decommission please recommission the nodes and repeat the above step with smaller number of nodes to decommission:</p> <p><code>kubectl exec -it cockroachdb-0 -- cockroach node recommission &lt;node id 1&gt; [&lt;node id 2&gt; ...] --certs-dir=cockroach-certs/</code></p> </li> <li> <p>Power down the pods or delete the Statefulset, whichever is applicable</p> <p>a. Again, Statefulsets does not support deleting specific pods, as it will    restart it immediately you will need to scale down understanding that it    will remove node <code>cockroachdb-n</code> first; where <code>n</code> is the    <code>size of statefulset - 1</code>.</p> <p>To proceed: <code>kubectl scale statefulset cockroachdb --replicas=&lt;X&gt;</code></p> <p>b. To remove the entire Statefulset: <code>kubectl delete statefulset cockroachdb</code></p> </li> </ol>"},{"location":"operations/pooling/","title":"DSS Pooling","text":"<p>[!WARNING] This document is about pooling with Yugabyte. CockroachDB documentation is there.</p>"},{"location":"operations/pooling/#introduction","title":"Introduction","text":"<p>The DSS is designed to be deployed in a federated manner where multiple organizations each host a DSS instance, and all of those instances interoperate. Specifically, if a change is made on one DSS instance, that change may be read from a different DSS instance.  A set of interoperable DSS instances is called a \"pool\", and the purpose of this document is to describe how to form and maintain a DSS pool.</p> <p>It is expected that there will be exactly one production DSS pool for any given DSS region, and that a DSS region will generally match aviation jurisdictional boundaries (usually national boundaries).  A given DSS region (e.g., Switzerland) will likely have one pool for production operations, and an additional pool for partner qualification and testing (per, e.g., F3411-19 A2.6.2).</p>"},{"location":"operations/pooling/#terminology-notes","title":"Terminology notes","text":"<p>Yugabyte establishes a distributed data store called a \"cluster\". This cluster stores the DSS Airspace Representation (DAR) in multiple SQL databases within that cluster.  This cluster is composed of many Yugabyte nodes, potentially hosted by multiple organizations.</p> <p>Kubernetes manages a set of services in a \"cluster\". This is an entirely different thing from the Yugabyte data store, and this type of cluster is what the deployment instructions refer to. A Kubernetes cluster contains one or more node pools: collections of machines available to run jobs. This node pool is an entirely different thing from a DSS pool.</p>"},{"location":"operations/pooling/#objective","title":"Objective","text":"<p>A pool of InterUSS-compatible DSS instances is established when all of the following requirements are met:</p> <ol> <li>Each Yugabyte node is addressable by every other Yugabyte node</li> <li>Each Yugabyte node is discoverable</li> <li>Each Yugabyte node accepts the certificates of every other node</li> <li>The Yugabyte cluster is initialized</li> </ol> <p>The procedures in this document are intended to achieve all the objectives listed above, but these procedures are not the only ways to achieve the objectives.</p>"},{"location":"operations/pooling/#each-yugabyte-node-is-addressable-by-every-other-yugabyte-node","title":"\"Each Yugabyte node is addressable by every other Yugabyte node\"","text":"<p>Every Yugabyte node must have its own externally-accessible hostname (e.g., 1.tserver.db.dss-prod.example.com), or its own hostname:port combination (e.g., db.dss-prod.example.com:26258).</p> <p>There are two type of nodes in a Yugabyte cluster: Master and TServer. Both ones must be accessible. The ports on which Yugabyte communicates must be open to others participants:</p> <ul> <li>Master: gRPC: 7100</li> <li>Master: Admin UI: 7000</li> <li>TServer: gRPC: 9100</li> <li>TServer: Admin UI: 9000</li> <li>TServer: ycql: 9042</li> <li>TServer: ysql: 5433</li> <li>TServer: metrics: 13000</li> <li>TServer: metrics: 12000</li> </ul> <p>The ports in bold are mandatory. The others ones are needed for management UI, the UI won't work correctly if any of those port is not reachable by other nodes.</p> <p>This requirement may be verified by conducting a standard TLS diagnostic (like this one) on the hostname:port for each TServer node (e.g., 0.tserver.db.dss.example.com:5433). The \"Trust\" characteristic will not pass because the certificate is issued by a custom CA which is not a generally-trusted root CA, but we explicitly enable trust by manually exchanging the trusted CA public keys in ca.crt (see \"Each Yugabyte node accepts the certificates of every other node\" below). However, all other checks should generally pass.</p> <p>NB: Only ports in bold and the 9042 are using TLS. You may test the others ones with your browser to check for connectivity.</p> <p>[!CAUTION] It's recommended to restrict access to those ports and only allow IPs of others participants. However guides and helm charts haven't been adapted yet.</p>"},{"location":"operations/pooling/#each-yugabyte-node-is-discoverable","title":"\"Each Yugabyte node is discoverable\"","text":"<p>When a Yugabyte node is brought online, it must know how to connect to the existing network of nodes. This is accomplished by providing an explicit list of nodes to contact. Each node contacted will provide a list of nodes it is connected to in the network (\"gossip\"), so not every node must be present in the explicit list, but it's recommended to do so. The explicit list shall contain, at a minimum, all known nodes when creating the DSS instance and shall be updated regularly. Yugabyte nodes have some difficulites to locate primary nodes if they don't have the full list of known nodes.</p>"},{"location":"operations/pooling/#each-yugabyte-node-accepts-the-certificates-of-every-other-node","title":"\"Each Yugabyte node accepts the certificates of every other node\"","text":"<p>Yugabyte uses TLS to secure connections, and TLS includes a mechanism to ensure the identity of the server being contacted.  This mechanism requires a trusted root Certificate Authority (CA) to sign a certificate containing the public key of a particular server, so a client connecting to that server can verify that the certificate (containing the public key) is endorsed by the root CA as being genuine. Yugabyte certificates require a claim that standard web CAs will not sign, so instead each USS acts as their own root CA.  When USS 1 is presented with certificates signed by USS 2's CA, USS 1 must know that it can trust that certificate.  We accomplish this by exchanging all USSs' CA public keys out-of-band in ca.crt, and specifying that certificates signed by any of the public keys in ca.crt should be accepted when considering the validity of certificates presented to establish a TLS connection between nodes.</p> <p>The private CA key <code>dss-certs.py</code> generates is stored in the <code>ca</code> folder. The private CA key is used to generate all node certificates and client certificates. Once a pool is established, a USS avoids regenerating this CA keypair, and use the existing ones by default. If a USS generates a new CA keypair, the new public key must be added to the pool's combined ca.crt, and all USSs in the pool must adopt the new combined ca.crt before any nodes using certificates generated by the new CA private key will be accepted by the pool.</p>"},{"location":"operations/pooling/#the-yugabyte-cluster-is-initialized","title":"\"The Yugabyte cluster is initialized\"","text":"<p>A Yugabyte cluster of databases is like the Ship of Theseus: it is composed of many nodes which may all be replaced, one by one, so that a given Yugabyte cluster eventually contains none of its original nodes.  Unlike the Ship of Theseus, however, a cluster is clearly identified by its cluster ID (e.g., b2537de3-166f-42c4-aae1-742e094b8349) -- if the cluster ID is the same, it is the same cluster (and vice versa). Once for the entire lifetime of the Yugabyte cluster, it is created automatically during initialization of the first set of Yugabyte nodes, if all initial nodes see each others in a uninitialized state.</p>"},{"location":"operations/pooling/#additional-requirements","title":"Additional requirements","text":"<p>These requirements must be met by every DSS instance joining an InterUSS-compatible pool.  The deployment instructions produce a system that complies with all these requirements, so this section may be ignored if following those instructions.</p> <ul> <li>All Yugabyte nodes must be run in secure mode.</li> <li>use_node_to_node_encryption enabled</li> <li>use_client_to_server_encryption enabled</li> <li>allow_insecure_connections disabled</li> <li>The ordering of the <code>--locality</code> flag keys must be the same across all   CockroachDB nodes in the cluster.</li> <li>All DSS instances in the same cluster must point their ntpd at the same NTP   Servers.</li> </ul>"},{"location":"operations/pooling/#creating-a-new-pool","title":"Creating a new pool","text":"<p>All DSS instances are equal peers, and any set of DSS instance can be chosen to create the pool initially. After the pool is established, additional DSS instance can join it. After that joining process is complete, it can be repeated any number of times to add additional DSS instances, though 7 is the maximum recommended number of DSS instances for performance reasons. The following diagram illustrates the pooling process for the first two instances:</p> <p></p> <p></p> <p>Adding participant is illustrated below. Some actions marked with <code>(once)</code> need to be run only once by one participant otherwise all participants in the current pool must ran then.</p> <p></p>"},{"location":"operations/pooling/#establishing-a-pool-with-first-instances","title":"Establishing a pool with first instances","text":"<p>The USSs owning the first DSS instances should follow the deployment instructions.</p> <p>Each DSS instance must set <code>yugabyte_external_nodes</code> with the list of each others DSS instance Yugabyte master nodes public endpoints, and CA certificates must be exchanged.</p> <p>It's possible to have one DSS instance as starting point. In that case, <code>yugabyte_external_nodes</code> will be empty and no CA exchange is needed.</p> <p>[!NOTE] Quick reminder for CA management:</p> <p>Each DSS instance should use <code>./dss-certs.sh init</code> To get the CA that should be sent to others instances, use <code>./dss-certs.sh get-ca</code> To import the CA of others DSS instance, use <code>./dss-certs.sh add-pool-ca</code> Finally, apply certificates on the kubernetes cluster with <code>./dss-certs.sh apply</code></p> <p>Ensure placement info is how you want it. See the section below for placement requirements.</p> <p>Upon deployment completion, the following should be run against the DSS instance to verify functionality:</p> <ul> <li>The prober test</li> <li>The USS qualifier,    using the DSS Probing configuration</li> </ul>"},{"location":"operations/pooling/#joining-an-existing-pool-with-new-instance","title":"Joining an existing pool with new instance","text":"<p>A USS wishing to join an existing pool (of perhaps just one instance following the prior section) should follow the deployment instructions.  They will be joining an existing cluster, and they will need to request all CAs that the pool is currently using (any one member of the pool may provide it). The joining USS will also need a list of Yugabyte node addresses.</p> <p>The joining USS must create his own CA with <code>./dss-certs.sh init</code> and retrieve it with <code>./dss-certs.sh get-ca</code>. This certificate must be provided to each existing DSS instance in the pool that will import it with <code>./dss-certs.sh add-pool-ca</code> and <code>./dss-certs.sh apply</code>.</p> <p>One of existing DSS instance shall provide to the joining USS all existing certificate, using <code>./dss-certs.sh get-pool-ca</code>. The joining USS can import them with <code>./dss-certs.sh add-pool-ca</code> and finally apply certificates with <code>./dss-certs.sh apply</code>. As an alternative, each DSS instance can provide its individual CA.</p> <p>Participants shall ensure they work with a coherent set of certificate by comparing the pool CA hash. It is displayed after adding certificates or using the <code>./dss-certs.sh list-pool-ca</code>.</p> <p>When CAs have been exchanged and configured everywhere, the joining participant can bring his system online (e.g. by applying helm charts onto his cluster). The <code>yugabyte_external_nodes</code> setting shall be set before starting the Yugabyte cluster.</p> <p>New nodes shall be allowed into the cluster. For each new Yugabyte master node, the following command shall be run on one master node of one existing DSS instance :</p> <p>[!WARNING] The <code>master_addresses</code> in all commands below must include the Yugabyte master leader. Either always run commands in the cluster with the leader, or list all public addresses.</p> <ol> <li> <p>Connection to a master node:</p> <p><code>kubectl exec -it yb-master-0 -- sh</code></p> </li> <li> <p>Preparation of certificates for the client:</p> <ul> <li><code>cp /opt/certs/yugabyte/node.yb-master-0.yb-masters.default.svc.cluster.local.crt /tmp/node.crt</code></li> <li><code>cp /opt/certs/yugabyte/node.yb-master-0.yb-masters.default.svc.cluster.local.key /tmp/node.key</code></li> <li><code>cp /opt/certs/yugabyte/ca.crt /tmp/ca.crt</code></li> </ul> </li> <li> <p>Addition of a new master node</p> <p><code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 change_master_config ADD_SERVER [PUBLIC HOSTNAME] 7100</code></p> </li> </ol> <p>The last command can be repeated as needed, however a small delay is needed for the cluster to settle when adding a new node. If you get <code>Leader is not ready for Config Change, can try again</code>, just try again.</p> <p>You should have all masters listed in the web ui or using the <code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 list_all_masters</code> command.</p> <p>The tserver nodes will join automatically, using the list of provided master nodes. They can be listed for confirmation in the web ui or using the <code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 list_all_tablet_servers</code> command.</p> <p>The pool should then be re-verified for functionality by running the prober test on each DSS instance, and the interoperability test scenario on the full pool (including the newly-added instance).</p> <p>Finally, the joining USS should provide its Yugabyte node addresses to all other participants in the pool, and each other participant should add those addresses to the <code>yugabyte_external_nodes</code> list their Yugabyte nodes will attempt to contact upon restart.</p> <p>Ensure placement info is how you want it. See the section below for placement requirements.</p>"},{"location":"operations/pooling/#leaving-a-pool","title":"Leaving a pool","text":"<p>In an event that requires removing Yugabyte nodes we need to properly and safely decommission to reduce risks of outages.</p> <p>It is never a good idea to take down more than half the number of nodes available in your cluster as doing so would break quorum. If you need to take down that many nodes please do it in smaller steps.</p> <p>Ensure placement info is how you want it after removal. Ensure you're not requesting impossible placement by removing nodes, otherwise it won't be possible to request node deletion. See the section below for placement requirements.</p> <p>Note: If you are removing a specific node in a Statefulset, please know that Kubernetes does not support removal of specific node; it automatically re-creates the node if you delete it with <code>kubectl delete pod</code>.  You will need to scale down the Statefulset and that removes the last node first (ex: <code>yb-tserver-n</code> where <code>n</code> is the <code>size of statefulset - 1</code>, <code>n</code> starts at 0)</p> <ol> <li> <p>Check if all nodes are healthy in the web ui.</p> </li> <li> <p>Connect to a Yugabyte master and copy certs, like introduced in the previous    section.</p> </li> <li> <p>For each TServer node to be removed:</p> <ol> <li>Blacklist one node in your cluster.</li> </ol> <p><code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 change_blacklist ADD [TSERVER_PUBLIC_HOSTNAME]</code></p> <ol> <li> <p>Wait for the node to be drained (no user tablet-peer or system-table-peer    in the gui). If node is not draining, you may have placement constraints    that prevent the removal of the node.</p> </li> <li> <p>Stop one node in your cluster.</p> </li> <li> <p>Wait until the node is marked as down and cluster will go into a    non-healthy state then wait for recovery. When everything is green again    proceed. Depending on settings, it may take time (15m) before the node is    marked as dead.</p> </li> <li> <p>Remove the node:</p> </li> </ol> <p><code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 remove_tablet_server [TSERVER_ID]</code></p> <p>If the command is giving you an error, data of the node may not have been drain correctly dues to placement constraints.</p> <ol> <li>Remove the node from the black list:</li> </ol> <p><code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 change_blacklist REMOVE [TSERVER_PUBLIC_HOSTNAME]</code></p> <ol> <li> <p>Fully remove the node in your cluster.</p> <p>E.g you may delete persistent volumes.</p> </li> </ol> </li> <li> <p>For each Master node to be removed:</p> <ol> <li>Remove the master from the master list</li> </ol> <p><code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 change_master_config REMOVE_SERVER [PUBLIC HOSTNAME] 7100</code></p> <p>If the master node to be removed is the current leader, you may make it step down with the following command:</p> <p><code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 master_leader_stepdown</code></p> </li> </ol> <p>Finally, each pool participant should remove master addresses from the <code>yugabyte_external_nodes</code> list their Yugabyte nodes will attempt to contact upon restart and remove the CA of the participant.</p> <p>[!NOTE] Quick reminder for CA management:</p> <p>Remove the old CA, use <code>./dss-certs.sh remove-pool-ca &lt;Certificate id&gt;</code> Finally, apply certificates on the kubernetes cluster with <code>./dss-certs.sh apply</code></p>"},{"location":"operations/pooling/#placement","title":"Placement","text":"<p>It's important to maintain a good placement strategy, ensuring data availability in case of failures.</p> <p>We do recommend a minimum of 3 participants and one copy in each participants.</p> <p>You may use the <code>modify_placement_info</code> command to set placement settings. Example:</p> <ul> <li><code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 modify_placement_info dss.uss-1,dss.uss-2,dss.uss-3 3</code></li> <li><code>yb-admin -certs_dir_name /tmp/ -master_addresses yb-master-0.yb-masters.default.svc.cluster.local:7100,yb-master-1.yb-masters.default.svc.cluster.local:7100,yb-master-2.yb-masters.default.svc.cluster.local:7100 modify_placement_info dss.uss-1,dss.uss-2,dss.uss-3,dss.uss-4,dss.uss-5 5</code></li> </ul> <p>You may however use a different strategy depending on your availability needs, e.g. you may want to avoid common datacenter between the same DSS instance. To do so, define a strategy in your pool and edit placement information as needed. More information is available in Yugabyte documentation.</p>"},{"location":"operations/troubleshooting/","title":"Troubleshooting","text":""},{"location":"operations/troubleshooting/#check-if-the-cockroachdb-service-is-exposed","title":"Check if the CockroachDB service is exposed","text":"<p>Unless specified otherwise in a deployment configuration, CockroachDB communicates on port 26257.  To check whether this port is open from Mac or Linux, e.g.: <code>nc -zvw3 0.db.dss.your-region.your-domain.com 26257</code>.  Or, search for a \"port checker\" web page/app.  Port 26257 will be open on a working CockroachDB node.</p> <p>A standard TLS diagnostic may also be run on this hostname:port combination and all results should be valid except Trust.  Certificates are signed by \"Cockroach CA\" which is not a generally-trusted CA, but this is ok.</p>"},{"location":"operations/troubleshooting/#accessing-a-cockroachdb-sql-terminal","title":"Accessing a CockroachDB SQL terminal","text":"<p>To interact with the CockroachDB database directly via SQL terminal:</p> <pre><code>kubectl \\\n  --context $CLUSTER_CONTEXT exec --namespace $NAMESPACE -it \\\n  cockroachdb-0 -- \\\n  ./cockroach sql --certs-dir=cockroach-certs/\n</code></pre>"},{"location":"operations/troubleshooting/#using-the-cockroachdb-web-ui","title":"Using the CockroachDB web UI","text":"<p>The CockroachDB web UI is not exposed publicly, but you can forward a port to your local machine using kubectl:</p>"},{"location":"operations/troubleshooting/#create-a-user-account","title":"Create a user account","text":"<p>Pick a username and create an account:</p> <p>Access the CockrachDB SQL terminal then create user with sql command</p> <pre><code>root@:26257/rid&gt; CREATE USER foo WITH PASSWORD 'foobar';\n</code></pre>"},{"location":"operations/troubleshooting/#access-the-web-ui","title":"Access the web UI","text":"<pre><code>kubectl -n $NAMESPACE port-forward cockroachdb-0 8080\n</code></pre> <p>Then go to https://localhost:8080. You'll have to ignore the HTTPS certificate warning.</p>"},{"location":"operations/ci/","title":"CI","text":""},{"location":"operations/ci/aws-1/","title":"AWS-1 CI deployment","text":"<p>This module deploys a DSS to a Kubernetes cluster in AWS. It is primarily by our CI. See test.sh for the complete list of actions.</p>"},{"location":"operations/ci/aws-1/#terraform-state","title":"Terraform state","text":"<p>The terraform backend is configured to be shared using a S3 bucket. (see <code>main.tf</code>).</p>"},{"location":"operations/ci/aws-1/#debugging","title":"Debugging","text":"<p>In case of issue, it is possible to connect to the cluster and retrieve the terraform state to manage it locally.</p>"},{"location":"operations/ci/aws-1/#connection-to-the-cluster","title":"Connection to the cluster","text":"<p>To connect to the cluster, authenticate yourself to the AWS account. Run the following command to load the kubernetes config: <pre><code>aws eks --region us-east-1 update-kubeconfig --name dss-ci-aws-ue1\n</code></pre> Call the kubernetes cluster using <code>kubectl</code></p>"},{"location":"operations/ci/aws-1/#add-other-roles","title":"Add other roles","text":"<p>Access to the cluster is managed using the config map <code>aws-auth</code>. Its definition is managed in <code>kubernetes_admin_access.tf</code>. Currently only the user who bootstrapped the cluster and the ones assuming the administrator role (see <code>local_variables.tf</code>) have access.</p>"},{"location":"operations/ci/aws-1/#run-terraform-locally","title":"Run terraform locally","text":"<p>In case of failure, a user with administrator role can take over the deployment by cloning this repository and retrieving the current deployment state by running the following command:</p> <pre><code>terraform init\n</code></pre> <p>At this point, the user can replay or clean the deployment as if it was the CI runner.</p>"},{"location":"services/","title":"Services","text":""},{"location":"services/helm-charts/","title":"DSS Helm Chart","text":"<p>This Helm Chart deploys the DSS and cockroachdb kubernetes resources.</p>"},{"location":"services/helm-charts/#requirements","title":"Requirements","text":"<ol> <li>A Kubernetes cluster should be running and you should be properly authenticated. Requirements and instructions to create a new Kubernetes cluster can be found here.</li> <li>Create the certificates and apply them to the cluster using the instructions of section 6 and 7</li> <li>Install Helm version 3.11.3 or higher</li> </ol>"},{"location":"services/helm-charts/#usage","title":"Usage","text":"<ol> <li>Copy <code>values.example.yaml</code> to <code>values.dev.yaml</code> and edit it. In particular, the key <code>dss.image</code> must be set manually. See <code>values.schema.json</code> for schema definition. The root key <code>cockroachdb</code> supports all values supported by the <code>cockroachdb</code> Chart). Note that values.yaml contains the default values and are always passed to helm.</li> <li>Validate the configuration: <code>helm lint -f values.dev.yaml .</code></li> <li>Set a RELEASE_NAME to <code>dss</code>: <code>export RELEASE_NAME=dss</code> It is temporarily the only release name possible.</li> <li>Set the kube client context of your system, example: <code>export KUBE_CONTEXT=gke_interuss-deploy-example_europe-west6-a_dss-dev-w6</code></li> <li>Run <code>helm dep update --kube-context=$KUBE_CONTEXT</code></li> <li>Install the chart: <code>helm install --kube-context=$KUBE_CONTEXT -f values.dev.yaml $RELEASE_NAME .</code></li> </ol>"},{"location":"services/helm-charts/#update-the-chart","title":"Update the chart","text":"<p>When changing the values in values.dev.yaml, values.yaml, the templates or upgrading the helm chart dependencies, changes can be applied to the cluster using the following command:</p> <ol> <li>Run <code>helm upgrade --kube-context=$KUBE_CONTEXT -f values.dev.yaml $RELEASE_NAME .</code></li> </ol>"},{"location":"services/tanka/","title":"Kubernetes deployment via Tanka","text":"<p>This folder contains a set of configuration files to be used by tanka to deploy a single DSS instance via Kubernetes following the procedures found in the build folder.</p>"},{"location":"services/tanka/#job-cleanup","title":"Job cleanup","text":"<p>Job are not automatically removed. It is possible to use tanka to delete unmanaged resources (eg previous jobs) by enabling the garbage collection feature in your <code>spec.json</code> file. Use <code>tk prune</code> to cleanup resources not present anymore in the jsonnet.</p>"}]}